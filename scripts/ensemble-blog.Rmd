---
title: "Real-time ensemble forecasting for influenza in the US"
author: Nick
layout: post
comments: TRUE
category: R
output:
  md_document:
    variant: markdown_github
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(dplyr)
library(readr)
library(ggplot2)
theme_set(theme_minimal())
```

In the 2017/2018 influenza season, the CDC ran the 5th annual [FluSight forecasting competition](https://www.cdc.gov/flu/weekly/flusight/index.html) and received 29 submissions from 20 teams. The FluSight Network collaborative ensemble model, a weighted combination of models based on past performance, was one of the top two most accurate models during this season, outperforming an ensemble model built by analysts at the CDC that combined all of the submitted models by taking the average forecast for each influenza target. The FluSight Network is continuing to generate real-time ensemble forecasts for influenza in this current 2018/2019 season. You can view our [online interactive forecasts](http://flusightnetwork.io/), which are updated weekly (typically on Monday evening).

<img class="img-responsive" width="700" src="/images/blog/">

<!--more-->

### A brief history of the FluSight Network

The FluSight Network is a  multi-institution and multi-disciplinary consortium including some of the teams that have participated in past [CDC FluSight forecasting challenges](https://www.cdc.gov/flu/weekly/flusight/index.html). In the 2017/2018 season the FluSight Network provided the second most accurate influenza forecasting model to the CDC, coming in narrowly behind an expert judgment forecasting model from Carnegie Mellon. A comparison of the performance of the 21 models submitted for the 2017/2018 has just been published in PNAS. A manuscript detailing the results of the ensemble forecast in 2017/2018 is forthcoming. Last year, we wrote [a blog post](http://reichlab.io/2017/11/28/flusight-ensemble.html) about the building of the ensemble model.

### The FluSight Network: the 2018/2019 edition

Prior to the start of the 2018/2019 influenza season in the US, we assembled 21 distinct forecasting models for influenza, each with forecasts from the last eight influenza seasons in the US. Subsequently, we conducted a cross-validation study to compare five different methods for combining these models into a single ensemble forecast. Across the past eight seasons, four of our collaborative ensemble methods had higher average scores than any of the individual forecasting models. In addition, last year our team's prospective forecasts were the second most accurate forecasts among 29 submitted to the CDC. Based on updated models for the 2018/2019 season, we chose the best performing ensemble model and are submitting forecasts from this model each week to the [CDC 2018/2019 FluSight Challenge](https://predict.cdc.gov/).


Here is a table showing the contributors to this year's ensemble:

Institution | No. of models | Team leaders
------------------------------ | ------------- | ---------------------------------------
Delphi team at Carnegie Mellon | 6 | Logan Brooks, Roni Rosenfeld
Columbia University | 7 | Teresa Yamana, Sasikiran Kandula, Jeff Shaman
Los Alamos National Laboratory | 1 | Dave Osthus, Reid Priedhorsky
Protea Analytics | 3 | Craig J. McGowan, Alysse J. Kowalski 
Reich Lab at UMass-Amherst | 4 | Nicholas Reich, Evan Ray, Katie House, Tom McAndrew, Nutcha Wattanachit


## Choosing an Ensemble Model for Real-time Influenza Forecasting

In late October 2018, the FluSight Network team chose a single model to submit to the CDC throughout the 2018/2019 influenza season. This model had the highest overall score among all individual component models and ensemble models examined (Figure 1). This model is called the "target weight" (TW) model because it assigns each model an individual weight for each target (see next section for details). 
Forecasts from this model have been, and continue to be, submitted to the CDC in real-time, starting on October 29, 2018. They may be viewed at a public website by visiting our https://flusightnetwork.io/.

We used an algorithm to estimate an optimal distribution of weights for the 21 different component models (Figure 2). 
For example, the `Springbok` model from the Protea Analytics team is given 45% of the weight when creating ensemble forecasts for the week of season onset and 12% of the weight for forecasts of 1-week-ahead incidence. 

While the weights are optimized to choose the best combination, they should not be interpreted as a ranking of models. For example, if two models make very similar forecasts but one is always a little bit better, it is possible that the slightly worse model will receive very little weight since most of the information it has to contribute is already contained in the better model. Typically, the ensemble will choose a set of models that contribute different information to the forecast, as this "diversity of opinion" will improve the forecast.


```{r, cache=TRUE}
scores <- read_csv("../scores/scores.csv")
models <- read_csv("../model-forecasts/component-models/model-id-map.csv")
complete_models <- c(models$`model-id`, 
    paste0("FSNetwork-", c("TW", "TTW", "TRW", "EW", "CW")))

## define column with scores of interest
SCORE_COL <- quo(`Multi bin score`)

## Create data.frame of boundary weeks of scores to keep for each target/season
source("create-scoring-period.R")
all_target_bounds = create_scoring_period()

## Remove scores that fall outside of evaluation period for a given target/season
scores_trimmed <- scores %>%
  dplyr::left_join(all_target_bounds, by = c("Season", "Target", "Location")) %>%
  dplyr::filter(`Model Week` >= start_week_seq, `Model Week` <= end_week_seq)


scores_adj <- scores_trimmed %>%
    filter(Model %in% complete_models) %>%
    ## if NA, NaN or <-10, set score to -10
    mutate(score_adj = dplyr::if_else(is.nan(!!SCORE_COL) | is.na(!!SCORE_COL) , 
                                      -10, 
                                      !!SCORE_COL)) %>%
    mutate(score_adj = dplyr::if_else(score_adj < -10 , -10, score_adj)) 

scores_by_season <- scores_adj %>%
    group_by(Model, Season) %>%
    summarize(
        avg_score = mean(score_adj),
        min_score = min(score_adj)
        ) %>%
    ungroup() %>%
    mutate(Model = reorder(Model, avg_score))
scores_by_model <- scores_adj %>%
    group_by(Model) %>%
    summarize(
        avg_score = mean(score_adj),
        min_score = min(score_adj)) %>%
    ungroup() %>%
    mutate(Model = reorder(Model, avg_score))

M <- 21
FSN_labels <- c("EW", "CW", "TTW", "TW", "TRW")
FSN_levels <- paste0("FSNetwork-", FSN_labels)
FNS_df <- M*c(0, 1, 2, 7, 77)
```

```{r, fig.cap="Average performance for all models, by season. The average forecast skill for each model within a season is plotted with a colored dot. The average across all seasons is shown with a black 'x'. Higher values indicate more accurate predictive performance, as they are a measure of how much probability on average a forecast from the given model assigned to the eventually observed value. The FluSightNetwork ensemble models are highlighted in red text. Models are sorted left to right in order of increasing accuracy.", fig.height=4}
ggplot(scores_by_season, aes(x=Model, y=exp(avg_score))) +
    geom_point(alpha=.5, aes(color=Season)) + 
    geom_point(data=scores_by_model, shape="x", size=1, stroke=5)+
    scale_color_brewer(palette="Dark2") +
    ylab("average forecast skill") +
        theme(axis.text.x = element_text(
        angle = 90, hjust = 1, vjust = .5,
        color=ifelse(
            levels(scores_by_season$Model)%in% FSN_levels,
            "red", 
            "black"
            ))
    ) 

```

```{r, eval=FALSE}
ensemble_scores_season <- scores_by_season %>% 
    filter(grepl("FSN", Model)) %>%
    mutate(Model = factor(Model, levels=FSN_levels, labels=FSN_labels))
ensemble_scores_model <- scores_by_model %>% 
    filter(grepl("FSN", Model)) %>%
    mutate(Model = factor(Model, levels=FSN_levels, labels=FSN_labels))
ggplot(ensemble_scores_season, aes(x=Model, y=avg_score)) +
    geom_point(alpha=.5, aes(color=Season)) + 
    geom_point(data=ensemble_scores_model, shape="x", size=1, stroke=5)+
    scale_color_brewer(palette="Dark2") +
    ylab("average log score") + xlab(NULL)+
    ggtitle("Average log-scores for ensemble models, by season")

```

```{r, fig.cap="Estimated model weights by target. The number in each cell corresponds to the weight assigned to the model in each column and the target in each row. The weights in each row sum to 1. These weights are used to create the weighted average ensemble model for the 2017/2018 season.", fig.height=3.5}
target_weights <- read.csv("../weights/target-based-weights.csv") %>%
    filter(season == "2018/2019") %>%
    mutate(component_model_id = reorder(component_model_id, weight))
levels(target_weights$target) <- list(
    "week ahead" = paste(4:1, "wk ahead"),
    "seasonal" = c("Season onset", "Season peak percentage", "Season peak week")
)

ggplot(target_weights, aes(y=target, fill=weight, x=component_model_id)) + 
    geom_tile() + ylab(NULL) + xlab(NULL) +
    geom_text(aes(label=round(weight, 2)), size=2.5) +
    scale_fill_gradient(low = "white", high="dodgerblue4", limits=c(0,1)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5), legend.position = "none") 
```



### Looking under the hood

__Targets:__ For every week in a season, each component model submission contains forecasts for seven targets of public health interest specified by the CDC for each of the 11 HHS regions. The region-level targets are: weighted influenza-like-illness (wILI) in each of the next four weeks of the season, the week of season onset, the week in which the peak wILI occurs, and the level of the peak wILI. Forecasts within 0.5 percentage points of the target wILI and within 1 week of the weekly targets are given full credit for having been "correct".

__Ensemble specifications:__ All of our ensemble models are built by taking weighted averages of the component models. We examined the performance of five different possible ensemble specifications (see table below). The "equal weights" model takes a simple average of all of the models, with no consideration of past performance. The other four approaches estimated weights for models based on past performance.

Model | No. of weights | description
------------------------------ | ----------- | -----------------------------------
Equal weights (EW) | 1 | Every model gets same weight.
Constant weights (CW) | 21 | Every model gets a single weight, not necessarily the same.
Target-type-based weights (TTW) | 42 | Two sets of weights, one for seasonal targets and one for weekly wILI targets.
Target-based weights (TW) | 147 | Seven sets of weights, one for each target separately.
Target-and-region-based weights (TRW) | 1,617 | Target-based weights estimated separately for each region.


__Forecast Evaluation:__ We measured performance by (1) comparing the average score across all targets and all relevant weeks in the last seven seasons and (2) comparing the variability in average score. The variability is important because a model can achieve good average performance by having a model that captures typical trends fairly well, but in a season showing unusual timing or dynamics it might fail. We want to ensure that we choose a model that shows good average performance but also is consistently good, especially in unusual seasons.

For submitting in real-time in 2018/2019, we selected the ensemble model that achieved the best overall score in a cross-validation experiment over the last seven seasons. This was the target-based model that assigned one set of weights to each component model for each target separately..


