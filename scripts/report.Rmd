---
title: 'Combining Mechanistic and Statistical Models to Forecat Influenza in the U.S.:  A Collaborative Ensemble from the FluSight Network'
author: Nicholas G Reich, Logan Brooks, Abhinav Tushar, Teresa Yamana, Craig McGowan, Evan Ray, Dave Osthus, Roni Rosenfeld
date: "November 2017"
output: pdf_document
header-includes:
- \renewcommand{\familydefault}{\sfdefault}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(dplyr)
library(readr)
library(ggplot2)
library(directlabels)
theme_set(theme_minimal())
```


## Overview

In the 2016/2017 influenza season, the CDC ran the 4th annual FluSight competition and received XX submissions from XX teams. During the season, analysts at the CDC built an ensemble model that combined all of the submitted models by taking the "average" forecast for each influenza target. This model was one of the top performing models for the entire season.

In March 2017 the FluSight Network, a collaborative group of influenza forecasters who have worked with the CDC in the past, was established to facilitate the pooling of resources to develop an ensemble that could incorporate past performance of models. This group worked throughout 2017 to create a set of guidelines and an experimental design that would enable submission of a publicly available, multi-team, real-time submission of an ensemble model with validated and performance-based weights for each model (i.e. not a simple average of models). 

This document provides an executive summary of that effort, highlighting the results and documenting the chosen model that was designated for real-time submission during the 2017/2018 U.S. influenza season.

Institution | No. of models | Team leaders
----------- | ------------- | -------------
Carnegie Mellon | 9 | Logan Brooks, Roni Rosenfeld
Columbia University | 7 | Teresa Yamana, Jeff Shaman
Los Alamos National Laboratories | 1 | Dave Osthus
UMass-Amherst | 4 | Nicholas Reich, Abhinav Tushar, Evan Ray

## Selected Ensemble Model for Real-time Submissions

For each week in a season, each model submits forecasts for seven targets of public health interest specified by the CDC for each of the 11 HHS regions. The region-level targets are: weighted influenza-like-illness (wILI) in each of the next four weeks of the season, the week of season onset, the week in which the peak wILI occurs, and the level of the peak ILI. Forecasts within 0.5 percentage points of the target wILI and within 1 week of the weekly targets are given full credit for having been "correct".

All of our ensemble models are built by taking weighted averages of the component models. The central question here is whether using past performance to determine those weights can improve our performance. We examined the performance of five different possible ensemble specifications. The first took a simple average of all of the models, with no consideration of past performance. The other four approaches estimated weights for models based on past performance. These four approaches varied in complexity, from having a single weight for each model to having weights for each model that varied by region and target. 

We measured performance by (1) comparing the average accuracy across relevant times and targets and (2) comparing the variability in performance. The variability is important because a model can achieve good average performance by having a model captures typical trends fairly well, but in a season showing unusual timing or dynamics it might fail. We want to ensure that we choose a model that shows good average performance but also is consistently good.


The model selected for real-time submissions is the model that showed the best overall performance in a cross-validation experiment over the last seven seasons. 


```{r, cache=TRUE}
scores <- read_csv("../scores/scores.csv")
models <- read_csv("../model-forecasts/component-models/model-id-map.csv")
complete_models <- c(models$`model-id`[models$complete=="true"], 
    paste0("FSNetwork-", c("TW", "TTW", "TRW", "EW", "CW")))

## define column with scores of interest
SCORE_COL <- quo(`Multi bin score`)

## Create data.frame of boundary weeks of scores to keep for each target/season
source("create-scoring-period.R")
all_target_bounds = create_scoring_period()

## Remove scores that fall outside of evaluation period for a given target/season
scores_trimmed <- scores %>%
  dplyr::left_join(all_target_bounds, by = c("Season", "Target", "Location")) %>%
  dplyr::filter(`Model Week` >= start_week_seq, `Model Week` <= end_week_seq)


scores_adj <- scores_trimmed %>%
    filter(Model %in% complete_models) %>%
    ## if NA, NaN or <-10, set score to -10
    mutate(score_adj = dplyr::if_else(is.nan(!!SCORE_COL) | is.na(!!SCORE_COL) , 
                                      -10, 
                                      !!SCORE_COL)) %>%
    mutate(score_adj = dplyr::if_else(score_adj < -10 , -10, score_adj)) 

scores_by_season <- scores_adj %>%
    group_by(Model, Season) %>%
    summarize(
        avg_score = mean(score_adj),
        min_score = min(score_adj)
        ) %>%
    ungroup() %>%
    mutate(Model = reorder(Model, avg_score))
scores_by_model <- scores_adj %>%
    group_by(Model) %>%
    summarize(
        avg_score = mean(score_adj),
        min_score = min(score_adj)) %>%
    ungroup() %>%
    mutate(Model = reorder(Model, avg_score))

M <- 21
FSN_labels <- c("EW", "CW", "TTW", "TW", "TRW")
FSN_levels <- paste0("FSNetwork-", FSN_labels)
FNS_df <- M*c(0, 1, 2, 7, 77)
```

```{r, fig.cap="Average performance for all models, by season. The average log-score for each model within a season is plotted with a colored dot. The average across all seasons is shown with a black 'x'. Higher values indicate more accurate predictive performance. The FluSightNetwork ensemble models are highlighted in red text. Models are sorted left to right in order of increasing accuracy."}
ggplot(scores_by_season, aes(x=Model, y=avg_score)) +
    geom_point(alpha=.5, aes(color=Season)) + 
    geom_point(data=scores_by_model, shape="x", size=1, stroke=5)+
    scale_color_brewer(palette="Dark2") +
    ylab("average log score") +
        theme(axis.text.x = element_text(
        angle = 90, hjust = 1, vjust = .5,
        color=ifelse(
            levels(scores_by_season$Model)%in% FSN_levels,
            "red", 
            "black"
            ))
    ) 

```



```{r, eval=FALSE}
ensemble_scores_season <- scores_by_season %>% 
    filter(grepl("FSN", Model)) %>%
    mutate(Model = factor(Model, levels=FSN_levels, labels=FSN_labels))
ensemble_scores_model <- scores_by_model %>% 
    filter(grepl("FSN", Model)) %>%
    mutate(Model = factor(Model, levels=FSN_levels, labels=FSN_labels))
ggplot(ensemble_scores_season, aes(x=Model, y=avg_score)) +
    geom_point(alpha=.5, aes(color=Season)) + 
    geom_point(data=ensemble_scores_model, shape="x", size=1, stroke=5)+
    scale_color_brewer(palette="Dark2") +
    ylab("average log score") +
    ggtitle("Average log-scores for ensemble models, by season")

```

## weights
```{r}
target_type_weights <- read.csv("../weights/target-type-based-weights.csv") %>%
    filter(target == "1 wk ahead" | target=="Season onset") %>%
    mutate(component_model_id = reorder(component_model_id, weight))

avg_target_type_weights <- target_type_weights %>% 
    group_by(target, component_model_id) %>% 
    summarize(weight = mean(weight)) %>% ungroup()
    
ggplot(avg_target_type_weights, aes(x=target, fill=weight, y=component_model_id)) + 
    geom_tile() + ylab(NULL) + xlab(NULL) +
    geom_text(aes(label=round(weight, 2))) +
    scale_fill_gradient(low = "white", high="dodgerblue4", limits=c(0,1)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    ggtitle("Average LOSO model weights by target")
```

