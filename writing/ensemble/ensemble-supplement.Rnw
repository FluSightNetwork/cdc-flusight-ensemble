\documentclass{article}
\usepackage[letterpaper, margin=1in]{geometry} % margin

\title{Supplement for\\``Accuracy of Real-Time Multi-Model Ensemble Forecasts for Seasonal Influenza in the U.S.''}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{parskip}        % for spacing after paragraphs http://ctan.org/pkg/parskip
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[noend]{algpseudocode}

\usepackage{setspace}
\onehalfspacing
\usepackage{lineno}% add line numbers
\linenumbers % line numbers


\author{Nicholas G Reich, Craig McGowan, Teresa Yamana, Abhinav Tushar, Evan L Ray,\\
Dave Osthus, Sasikiran Kandula, Logan Brooks\\
Willow Crawford-Crudell, Graham Casey Gibson, Evan Moore, Rebecca Silva\\Matthew Biggerstaff, Michael A Johansson, Roni Rosenfeld, Jeffrey Shaman}



\begin{document}

\maketitle


<<setup, echo=FALSE, warning=FALSE, message=FALSE>>=
knitr::opts_chunk$set(
  echo = FALSE, cache.path = './cache', message = FALSE, warning = FALSE
)
library(readr)
library(dplyr)
library(reshape2)

specify_decimal <- function(x, k=0) trimws(format(round(x, k), nsmall=k))
@

\section{Component models}
See Table \ref{tab:model-list}.

\input{./static-content/model-table.tex}

% \section{Forecast accuracy by region and target}
% 
% See Figure \ref{fig:regional-performance}.
% \begin{figure}[htbp]
% \begin{center}
% \includegraphics{figure/rt-scores-region-target-1.pdf}
% \caption{Average forecast scores and ranks by target and region for 2017/2018. Only selected models are shown. Regions are sorted with the most predictable region overall (i.e. highest forecast scores) at the top. Color indicates model rank in the 2017/2018 season (darker color indicates better average score). The average forecast score is printed.}
% \label{fig:regional-performance}
% \end{center}
% \end{figure}

\section{Supplemental evaluation metrics}

\subsection{Bias and MSE}

We compared the {\tt FSNetwork-TTW} ensemble model's accuracy in 2017/2018 to the top-performing models from each team in the training phase. 
We used the metrics of root mean-squared error (RMSE) and average bias to measure accuracy of point estimates.
Note that the ensemble weights were optimized solely to maximize log-score, so these accuracy scores are not an indicator of how well the ensemble could do if it were optimized to minimize point-estimate error.
Consistent with the CDC scoring rules, we only evaluated point estimates within the ``scoring bounds'' specific to each target, region, and season (see Methods in main manuscript).

Overall, the {\tt FSNetwork-TTW} model ranked second in both RMSE and average bias, behind the {\tt LANL-DBM} model (Figure \ref{fig:bias-rmse}).
All selected models showed a negative bias (i.e. underestimation, on average) of the targets on the wILI scale (week-ahead incidence and peak percentage).
The {\tt CU-EKF\_SIRS} model showed particularly low bias for 1- and 2-week-ahead foreacasts, although greater variability led to lower ranks for RMSE.

In general, these results suggest that using separate weighting schemes for point estimates and predictive distributions may be valuable.

\begin{figure}[htbp]
\begin{center}
\includegraphics{figure/rmse-and-bias-1.pdf}
\caption{Root mean squared error (RMSE, panel A) and bias (panel B) by target for selected models (with rank) in the 2017/2018 season. Evaluations are for all weeks in the 2017/2018 season. Models are sorted with lowest RMSE on right.}
\label{fig:bias-rmse}
\end{center}
\end{figure}



\subsection{Probability integral transform}

The Probability Integral Transform (PIT) is an evaluation metric that can be used to assess the calibration of a predictive model.
A common application of PIT is testing whether a set of values from an unknown target distribution can be accurately modeled by one or more predictive distributions. 
In brief, statistical theory tells us that if we plug the set of observed values (which come from the unknown target distribution) into the cumulative distribution function of the predicted distribution, the output, otherwise known as the PIT values, should be uniformly distributed if the predicted distribution matches the true distribution.\cite{angus1994probability,diebold1997evaluating}
Therefore, looking at the PIT values provides a quantitative and qualitative assessment of the predictive model calibration by comparing the shape of the histogram of PIT values to a uniform distribution. 
Intuitively, the PIT measures how often a model's probabilistic assessment is true, i.e. does an event that the model says has a 10\% chance of occuring really only occur 10\% of the time.
Systematic deviations from the expected uniform distribution may indicate lack of calibration in some aspects of the predictive model.

In our application, we evaluate the set of all probabilistic forecasts of the five targets on the wILI scale (1 through 4 week-ahead wILI percent and the peak percentage) from the {\tt FSNetwork-TTW} model using PIT. For the 2017/2018 influenza season, we obtained a PIT value from each predictive distribution based on region, target, and week of season. 
As in other evaluations presented here, we only considered forecasts from the time-period of interest for the CDC, depending on the timing of the peak for each region-season.
We rounded each PIT value to the nearest tenth of a decimal place and plotted them on a historgram with ten bars, one for each decile of the Uniform(0,1) distribution.
We computed a Monte Carlo confidence interval under the null hypothesis that the PIT values are independent and identical draws from a Uniform(0,1) distribution, conditional on the number of PIT values. 

<<pit-2017-2018, fig.cap="Probability integral transform histograms by target for the FSNetwork-TTW model in the 2017/2018 season. If the model is well-calibrated, the histogram should resemble a uniform distribution, i.e. all the bars should be level at $y$=1. The red dashed lines represent a Monte Carlo confidence interval under the null hypothesis that the PIT values follow a Uniform(0,1) distribution. The fact that some of the bars for the season peak percentage target lie outside the CI bounds suggest that the model shows significantly weak calibration for that particular target.", cache=TRUE>>=
#1. data loading (applying code in data loading .Rmd to season 2017-2018) 
#ie use some predefined vectors and 'make_tidy()' function in previous Rmd

#new vectors 
valid_weeks_new<- c('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52')
later_weeks_new<- c('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18')
first_weeks_new<- c('43', '44', '45', '46', '47', '48', '49', '50', '51', '52')
#seasons<- c('2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017')
weeks_as_ints_new<- c(1:18, 43:52)


import_new<- function(path, start_wk, start_yr){     
  data<- rbind()
  for(week in valid_weeks_new){
    for (year in 2017:2018){
      if (!(week %in% later_weeks_new & year =='2017') & !(week %in% first_weeks_new & year =='2018')){
        str<- sprintf(path, week, year)
        new<- read_csv(str)
        new$week<- rep(as.numeric(substr(str,start_wk,start_wk+1)), nrow(new)) #adds column: week 
        new$year<- rep(as.numeric(substr(str,start_yr, start_yr+3)),nrow(new)) #adds column: year 
        data<- rbind(data, new)}
    }
  }
  return(data)
}

make_tidy<- function(ensemble){
  tidy<- ensemble %>% filter(location !=  'US National', target != 'week') %>% mutate(Season = ifelse(week > 30, paste(year, year+1, sep = "/"), paste(year-1, year, sep= "/")))
  return(tidy)
}

## 80, 83 based on character start of week and year in filename.
target_type_weights17_18<-import_new('../../model-forecasts/real-time-ensemble-models/target-type-based-weights/EW%s-%s-target-type-based-weights.csv', 77, 80)

target_type_weights_tidy17_18<- make_tidy(target_type_weights17_18)

#load true values from Season17-18
true_with_17_18<- read_csv('../../scores/target-multivals-20172018.csv')
names(true_with_17_18) <- gsub(" ", "_", names(true_with_17_18))
true_tidy_with_17_18<- true_with_17_18 %>% 
  filter(Season == '2017/2018', Location != 'US National', Target %in% c('1 wk ahead', '2 wk ahead', '3 wk ahead', '4 wk ahead', 'Season peak percentage'))

#gets true observation from dataframe true_tidy_with_17_18 (get_true_new is same as get_true w/o accounting for various seasons) 
get_true_new<- function(CW, Loca, Tar){   
  obs <- true_tidy_with_17_18 %>% filter(Calendar_Week == CW, Location == Loca, Target==Tar) %>% select(Valid_Bin_start_incl)
  return(as.numeric(obs))
}

#empirical cdf function
ecdf_round<- function(q, values){ 
  d<- round(q,1)
  bin<- d*10
  return(cumsum(values)[bin])
} 

#uses empirical cdf function to find PIT val for each week, target, and region
generate_pit_vals<- function(ensemble_data){
  regions <- unique(ensemble_data$location)
  targets <- unique(ensemble_data$target)
  pit_values<- c()
    #for (s in seasons){
      for (w in weeks_as_ints_new){
        for (r in regions){
          for(t in targets){
            df<- ensemble_data %>% filter(location== r, target== t, week==w) 
            percents <- df$value[1:length(df$value)-1]
            q<-get_true_new(w,r,t)
            pit_values<- c(pit_values,ecdf_round(q, percents))
          #}
        }
      }
    }
  return(pit_values)
}

get_CI<- function(N, breaks){
  set.seed(2)
  m<- c()
  for(i in 1:1000){
    r<- rnorm(N, 0, 1)
    z<- pnorm(r)
    list_hist<-hist(z, breaks=breaks, freq=F, plot=F)
    m<- c(m, max(list_hist$density))
  }
  max(m)
  m_alpha<- max(m)*(1-.05/2)
  return(m_alpha)
}
  
#all pit for season: 28 weeks * 10 regions * 5 targets = 1,400
upper_all<- get_CI(1400,10)[1] #1.330179
lower_all<- 1- (upper_all- 1)

#by target: 28 weeks *10 regions= 280
upper_tar<- get_CI(280,10)[1] 
lower_tar<- 1- (upper_tar- 1)

#by region: 28 weeks * 5 targets = 140
upper_reg<- get_CI(140,10)[1] 
lower_reg<- 1- (upper_reg- 1)

#target-type
# all_ttw_s1718<- generate_pit_vals(target_type_weights_tidy17_18)
# par(mfrow=c(1,2))
# hist(all_ttw_s1718, freq=F, main= "Target-type Weights\n PIT Histogram", xlab= "PIT values", breaks=10)
# abline(h=c(upper_all, lower_all), col= 2, lty= 5)


target_pit_values_new2<- function(model, tar){ 
    #takes various seasons out of 'target_pit_values_new'
    regions <- unique(model$location)
    pit_values<- c()
    for (w in weeks_as_ints_new){
        for (r in regions){
          df<- model %>% filter(location== r, target== tar, week==w) 
          percents <- df$value[1:length(df$value)-1]
          q<-get_true_new(w,r,tar)
          pit_values<- c(pit_values,ecdf_round(q, percents))
      }
    }
  return(pit_values)
}

tar_type_new2<- list()

targets<- c('1 wk ahead', '2 wk ahead','3 wk ahead','4 wk ahead','Season peak percentage')
i<- 1
for (t in targets){
    tar_type_new2[[i]]<- target_pit_values_new2(target_type_weights_tidy17_18, t)
    i<- i+1
}

par(mfrow=c(2,3))
for(i in 1:5){
  hist(tar_type_new2[[i]], freq=F, ylim= c(0, 2.7), main= (paste(targets[i])), xlab='PIT values')
  abline(h=c(upper_tar, lower_tar), col= 2, lty= 5)
  abline(h=1, col=4, lty=3)
}
@


<<pit-all-seasons, cache=TRUE, fig.cap="Probability integral transform histograms by target for the FSNetwork-TTW model in all training seasons: 2010/2011 through 2016/2017. If the model is well-calibrated, the histogram should resemble a uniform distribution, i.e. all the bars should be level at $y$=1. The red dashed lines represent a Monte Carlo confidence interval under the null hypothesis that the PIT values follow a Uniform(0,1) distribution. The intervals are narrower here than in Figure \\ref{fig:pit-2017-2018} because there are more observations from all the training seasons combined than in the one testing season. The model is showing some significant lack of calibration for all targets. In particular for the season peak percentage, substantially more observations were in the lowest 10\\% of the predictive distributions than would have been expected due to chance.">>=

valid_weeks<- c('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52')
later_weeks<- c('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20')
first_weeks<- c('43', '44', '45', '46', '47', '48', '49', '50', '51', '52')
seasons<- c('2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017')
regions<- c('HHS Region 1', 'HHS Region 2','HHS Region 3','HHS Region 4','HHS Region 5','HHS Region 6','HHS Region 7','HHS Region 8','HHS Region 9','HHS Region 10')
targets<- c('1 wk ahead', '2 wk ahead','3 wk ahead','4 wk ahead','Season peak percentage')
weeks_as_ints<- c(1:20, 43:52)


import<- function(path, start_wk, start_yr){     
  data<- rbind()
  for(week in valid_weeks){
    for (year in 2010:2017){
      if (!(week %in% later_weeks & year =='2010') & !(week %in% first_weeks & year =='2017')){
        str<- sprintf(path, week, year)
        new<- read_csv(str)
        new$week<- rep(as.numeric(substr(str,start_wk,start_wk+1)), nrow(new)) #adds column: week 
        new$year<- rep(as.numeric(substr(str,start_yr, start_yr+3)),nrow(new)) #adds column: year 
        data<- rbind(data, new)}
    }
  }
  return(data)
}

target_type_weights_for_pit <- import('../../model-forecasts/cv-ensemble-models/target-type-based-weights/EW%s-%s-target-type-based-weights.csv', 70, 73)

make_tidy<- function(ensemble){
  tidy<- ensemble %>% filter(location !=  'US National', target != 'week') %>% mutate(Season = ifelse(week > 30, paste(year, year+1, sep = "/"), paste(year-1, year, sep= "/")))
  return(tidy)
}

target_type_weights_tidy <- make_tidy(target_type_weights_for_pit)

#load true values from Season17-18
true<- read_csv('../../scores/target-multivals.csv')
names(true) <- gsub(" ", "_", names(true))
true_tidy<- true %>% 
  filter(Location != 'US National', Target %in% c('1 wk ahead', '2 wk ahead', '3 wk ahead', '4 wk ahead', 'Season peak percentage'))


get_true<- function(CW, Seas, Loca, Tar){   
  obs <- true_tidy %>% filter(Calendar_Week == CW,Season== Seas, Location == Loca, Target==Tar) %>% select(Valid_Bin_start_incl)
  return(as.numeric(obs))
}


target_pit_values <- function(model, tar){
    #takes various seasons out of 'target_pit_values_new'
    regions <- unique(model$location)
    seasons <- unique(model$Season)
    pit_values<- c()
    for (w in weeks_as_ints_new){
        for (r in regions){
            for(s in seasons) {
                df<- model %>% filter(location== r, target== tar, week==w, Season==s)
                percents <- df$value[1:length(df$value)-1]
                q<-get_true(w, s, r, tar)
                pit_values<- c(pit_values,ecdf_round(q, percents))
            }
        }
    }
  return(pit_values)
}


#by target: 210 weeks *10 regions= 2100
upper_tar_all <- get_CI(2100, 10)[1] 
lower_tar_all <- 1- (upper_tar_all- 1)

tar_type <- list()

for (i in 1:length(targets)){
    tar_type[[i]]<- target_pit_values(target_type_weights_tidy, targets[i])
}

par(mfrow=c(2,3))
for(i in 1:5){
  hist(tar_type[[i]], freq=F, ylim= c(0, 2.5), main= (paste(targets[i])), xlab='PIT values')
  abline(h=c(upper_tar_all, lower_tar_all), col= 2, lty= 5)
  abline(h=1, col=4, lty=3)
}

@

In the 2017/2018 season, the {\tt FSNetwork-TTW} model showed good calibration for all week-ahead targets (Figure \ref{fig:pit-all-seasons}). 
The models appeared to be slightly better calibrated for shorter forecast horizons (i.e. 1- and 2-week ahead) than for longer horizons. 
Forecasts for the peak percentage were less well calibrated, with more forecasts than expected occuring in both low and high tails of the predictive distribution.

Across all training seasons, the {\tt FSNetwork-TTW} model showed some lack of calibration for all targets considered (Figure \ref{fig:pit-2017-2018}).
In particular, the predictive distributions appeared to be too wide, with eventually observed values falling in the central region of the distribution more than expected. 
A slight negative bias is evident as well, from the skewness of the PIT figures, with the observed values more likely to fall under the median of the distribution for 2-, 3-, and 4-week-ahead forecasts.
However, these forecasts were evaluated on only seven seasons worth of data, and given that the forecasts were better calibrated in a larger-than-usual season such as 2017/2018 (Figure \ref{fig:pit-all-seasons}) suggests that the model in the training phase may have been appropriately allowing for the possibility of such a large season.

All in all, there could be some improvement in model calibration as measured by PIT. 
However, to date, this has not been designated as an explicit target for optimization of the ensemble weighting schemes.



\section{Updated weights incorporating 2017/2018 performance}

See Figure \ref{fig:updated-weights}.

\begin{figure}[htbp]
\begin{center}
\includegraphics{figure/updated-weights-1.pdf}
\caption{The change in the estimated weight for each model after including the 2017/2018 performance results.}
\label{fig:updated-weights}
\end{center}
\end{figure}

\clearpage

\section{EM Algorithm for Weighted Density Ensembles}

The following is adapted from \cite{Rosenfeld1997,Rosenfeld2007} and describes the use of the Expectation Maximization (EM) algorithm for constructing weighted average ensemble models in the context of infectious disease forecasting.  
Our goal is to develop a weighted density ensemble that combines the full predictive distributions in such a way as to optimize the score of the resulting model average.


To use the EM algorithm to find optimal weights, we formulate the question as a missing data problem. 
We consider a data generating process in which an observed target is generated from $f(z)$ by choosing one of the $f_c(z)$ component distributions as a random draw from a multinomial distribution with probabilities $\pi_c$. 
Here we supress the subscripts for target, region and week ($t$, $r$, and $w$) for simplicity.
The problem is that we do not know, for each observed datapoint $z_i^*$, which component this observation was drawn from. 
However, we can make a best guess, conditional on the data and our current estimates of $\pi_c$, of how often each component was chosen. 
This is the ``E step''. Then, based on these guesses, we can update our estimate of $\pi_c$. This is the ``M-step''.

The ``E step'' of the EM algorithm we can think of as determining, for each component  $c$, the expected number of times for each of our observed $N$ datapoints that component $c$ was chosen as the contributor to $f(z)$:
\begin{eqnarray}
{\mathbf E}[ {\mbox model}_c |data ] % & = & \sum_i Pr(Model_c | z_i^*) \\ 
 %& = & \sum_i \frac{Pr(Model_c, z_i^*)}{Pr(z_i^*)} \\ 
 %& = & \sum_i \frac{Pr(Model_c) Pr(z_i^*|model_c)}{f(z_i^*)} \\ 
 & = & \sum_i \frac{\pi_c f_c(z_i^*)}{f(z_i^*)} 
\end{eqnarray}
Heuristically, we can think of the expression $f_c(z)$ equivalently as $Pr(z| model c)$ or in words the likelihood of seeing the value $z$ given that component $c$ is the ``chosen'' model. 

The ``M step'' of the EM algorithm simply calculates, conditional on the ``complete data'', i.e. the $z^*_i$ and the estimated number of times each component was chosen, the fraction of times each method was chosen. Therefore, if we simply divide the quantity from the ``E-step'' by $N$, our total number of observations, we obtain a new estimate of this probability: 
\begin{eqnarray}
\pi_c^{(k+1)} & = & \frac{1}{N} {\mathbf E}[ {\mbox model}_c |data ] \\
& = & \frac{1}{N} \sum_i \frac{\pi^{(k)}_c f_c(z_i^*)}{f(z_i^*)} 
\end{eqnarray}

Assume that we have a set of $C$ fitted predictive densities ``evaluated at'' observed data  $z_i^*$ for $i=1, ..., N$.
In our application, we let the $f_c(z_i^*)$ be computed as the probabilities associated with the modified scores as described in the main manuscript.
As an example, for season peak percentage and the short-term forecasts, probabilities assigned to wILI values within 0.5 units of the observed values are included as correct, so the modified score becomes $f_c(z_i^*) = \int_{z_i^* -.5}^{z_i^* + .5} f_c(z|{\bf{x}})dz$.
We will notate these scores as  $f_{c}(z_i^*|{\bf x})$. There will be $C\cdot N$ total observations, as each model must have an associated score (a probability, between 0 and 1) for each observed data point.

We wish to obtain a set of optimal weights $\tilde\pi = \{\tilde\pi_1, \tilde\pi_2, ..., \tilde\pi_C\}$ for combining the models such that $\forall c$ $\tilde\pi_c \geq 0$ and $\sum_{c=1}^C \tilde\pi_c=1$.
The weights can be used to then combine the component models into an ensemble model as
$$f(z|\pi) = \sum_{c=1}^C \pi_c f_c(z).$$
We define a function $\ell(\pi)$ that computes a log-likelihood of the resulting ensemble as follows:
$$\ell(\pi) = \frac{1}{N}\sum_{i=1}^N \log f(z_i|\pi).$$

Below, we define one procedure to obtain a set of weights for the ensemble.

\begin{algorithm}
\caption{Degenerate Expectation Maximization (DEM) algorithm}\label{alg:DEM}
\begin{algorithmic}[1]
\Procedure{dem}{$...$}%\Comment{The g.c.d. of a and b}
\State Initialize $\pi_c^{(0)}$ such that $\forall c$ $\pi_c^{(0)} \geq 0$ and $\sum_{c=1}^C \pi_c^{(0)}=1$ 
\State Set $t=0$
\State Set $\Delta=1$, or another arbitrary constant.
\State Set $\epsilon$ to be a very small positive number strictly less than $\Delta$.
\While{$ \Delta > \epsilon$}%\Comment{We have the answer if r is 0}
\State Set $t=t+1$
\State Update weights, $\forall c$, $\pi_c^{(t)} = \frac{1}{N}\sum_{i=1}^N \frac{\pi_c^{(t-1)}f_c(z_i)}{f(z_i|\pi^{(t-1)})}$
\State Set $\Delta =  \frac{\ell(\pi^{(t)}) - \ell(\pi^{(t-1)})}{|\ell(\pi^{(t)})|}$ \label{delta-step}
\EndWhile
\State \textbf{return} $\tilde\pi = \tilde\pi^{(t)}$%\Comment{The gcd is b}
\EndProcedure
\end{algorithmic}
\end{algorithm}

And note that in Algorithm \ref{alg:DEM}, Step \ref{delta-step} it should always be the case that $\ell(t) \geq \ell(t-1)$.

We note that this application of the EM algorithm is a very simple example of the standard EM, which in general does not guarantee a global maximum. 
However, this particular log-likelihood function (a sum of logarithms of weighted sums) is a convex function of its parameters, the $\pi_c$, and convexity ensures that any local maximum is a global maximum.  
In particular, we can ensure that there is a global, finite maximum and that the EM finds it by including a uniform component and making the algorithm start with all nonzero weights.


\bibliographystyle{unsrt}
\bibliography{../flusightnetwork.bib}


\end{document}