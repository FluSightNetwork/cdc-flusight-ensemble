\documentclass{article}

\title{A Collaborative Ensemble Approach to Real-Time Influenza Forecasting in the U.S.: Results from the 2017/2018 Season}

\author{Nicholas G Reich$^1$, Logan Brooks$^2$, Sasikiran Kandula$^3$, Craig McGowan$^4$,\\Evan Moore$^1$, Dave Osthus$^5$, Evan Ray$^6$, Abhinav Tushar$^1$, Teresa Yamana$^3$, \\Matthew Biggerstaff$^4$, Michael A Johansson$^7$, Roni Rosenfeld$^2$, Jeffrey Shaman$^3$}

\date{%
    $^1$University of Massachusetts-Amherst, Amherst, USA\\%
    $^2$Carnegie Mellon University, Pittsburgh, USA\\%
    $^3$Columbia University, New York, USA\\
    $^4$Influenza Division, Centers for Disease Control and Prevention, Atlanta, USA\\
    $^5$Los Alamos National Laboratory, Los Alamos, USA\\
    $^6$Mount Holyoke College, South Hadley, USA\\
    $^7$Division of Vector-Borne Diseases, Centers for Disease Control and Prevention, Atlanta, USA\\
}
\usepackage[letterpaper, margin=1in]{geometry} % margin
\usepackage{lineno}% add line numbers
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{parskip}        % for spacing after paragraphs http://ctan.org/pkg/parskip
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath, amsfonts}
\usepackage{setspace}
\linenumbers % line numbers
\onehalfspacing



% For computer modern sans serif
\usepackage[T1]{fontenc}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif


\begin{document}

%\SweaveOpts{concordance=TRUE}

\maketitle

\tableofcontents

<<echo=FALSE, warning=FALSE, message=FALSE>>=
knitr::opts_chunk$set(
  echo = FALSE, cache = FALSE, cache.path = './cache', message = FALSE, warning = FALSE
)
library(dplyr)
library(readr)
library(ggplot2)
library(MMWRweek)
library(xtable)
library(cdcfluview)
library(gridExtra)

theme_set(theme_minimal())
specify_decimal <- function(x, k=0) trimws(format(round(x, k), nsmall=k))
@


\clearpage

\begin{abstract}
TBD.
\end{abstract}

\section{Introduction}

Outline
\begin{itemize}
    \item Ensembles are good - weather and inf. disease.
    \item Simple ensembles ok, evidence weighted ensembles better
    \item CDC forecasting flu for a while - history of challenge means number of teams with models to potentially combine
    \item Formation of FluSight Network
    \item Overview of goals, methods, etc.
\end{itemize}

Ensemble models, or models that fuse together predictions from multiple different models, have long been seen as a valuable method for improving predictions over any single model. This "wisdom of the crowd" approach (where the "crowd" can be thought of as a throng of models) has both theoretical and practical advantages. First, it allows for an ensemble forecast to incorporate signals from different data sources and models that may highlight different features of a system. Second, combining signals from models with different biases may allow those biases to offset and result in an ensemble that is more accurate than the individual component models. Weather and climate models have utilized ensemble systems for these very purposes, and recent work has extended ensemble forecasting to forecasts of infectious diseases, including influenza , dengue fever, and Ebola hemorrhagic fever %\cite{Viboud2018}.

Since the 2013/2014 influenza season, the Centers for Disease Control and Prevention (CDC) has run an annual prospective influenza forecasting competition, known as the FluSight challenge, in collaboration with outside researchers. Participating teams submit probabalistic forecasts for a variety of influenza targets weekly from early November through mid May. During the 2015/2016 and 2016/2017 challenges, analysts at the CDC built a simple ensemble model by taking the arithmetic mean of submitted models. This model was one of the top performing models each season (cite McGowan et al when accepted).

Given the success of a simple ensemble that incorporated no information about the relative performace of component models, an ensemble taking component model performance into account has the potential for further improvments. In March 2017 the FluSight Network, a collaborative group of influenza forecasters who have worked with the CDC in the past, was established to facilitate the pooling of resources to develop an ensemble that could incorporate past performance of models. This group worked throughout 2017 to create a set of guidelines and an experimental design that would enable submission of a publicly available, multi-team, real-time submission of an ensemble model with validated and performance-based weights for each model. 

This paper describes the development of 




% This document provides an executive summary of that effort, highlighting the results and documenting the chosen model that was designated for real-time submission during the 2017/2018 U.S. influenza season.
% 
% Institution | No. of models | Team leaders
% ----------- | ------------- | -------------
% Carnegie Mellon | 9 | Logan Brooks, Roni Rosenfeld
% Columbia University | 7 | Teresa Yamana, Jeff Shaman
% Los Alamos National Laboratories | 1 | Dave Osthus
% UMass-Amherst | 4 | Nicholas Reich, Abhinav Tushar, Evan Ray
% 
% 
%  Selected Ensemble Model for Real-time Submissions

% The model selected for real-time submissions is the model that performed 




\section{Results}

\subsection{Summary of component models}

\begin{itemize}
    \item General description of models included, targets, regions, etc. 
    \item Describe observed variation in performance across training seasons
    \item Quantitative summaries of how much forecasting data collected and how stored/scored
\end{itemize}

\subsection{Target-type weights selected based on cross-validation}
\begin{itemize}
    \item Ensembles in general showed more accuracy than component models
    \item Target-type weights achived highest cross-validated scores
    \item Weights distributed across models from all teams
\end{itemize}


\subsection{Summary of ensemble real-time performance in 2017/2018 season}
\begin{itemize}
    \item Comparison of ensemble models to non-ensemble models in 2017/2018 and comparison within ensembles
    \item Summary of performance (reduced accuracy) in anomalous 2017/2018 season
    \item post-hoc analysis of other possible weighting schemes
\end{itemize}


\section{Discussion}

Outline
\begin{itemize}
  \item Mostly TBD depending on results
  \item Comparisons to unweighted average
  \item Comparisons to component models
  \item Strengths and weaknesses of ensemble approach
  \item Practicality and real-world impacts
\end{itemize}


\section{Methods}

Outline
\begin{itemize}
    \item ILI definition
    \item Forecast structure
    \item Forecast evaluation
    \item Component models
    \item Ensemble construction
\end{itemize}

\subsection{Influenza Data}
Forecasting targets for the CDC FluSight challenge are based on the US Outpatient Influenza-like Illness Surveillance Network (ILINet). ILINet is a syndromic surveillance system that measures the weekly percentage of outpatient visits due to influenza-like illness (ILI) from a network of more the 2,800 providers, and publishes a weighted estimate of ILI (wILI) based on state populations. Estimates of wILI are reported weekly by the CDC's Influenza Division for the United States as a whole as well as for each of the 10 Health and Human Services (HHS) regions. Reporting of 'current' wILI is typically delayed by approximately two weeks as data are collected and processed, and each weekly publication can also include revisions of prior reported values if new data become available. For the US and each HHS Region, CDC publishes an annual baseline level of ILI activity based on off-season ILI levels. 

\subsection{Forecast Targets and Structure}
Forecasts for the CDC FluSight challenge consist of seven targets, three seasonal targets and four short-term targets. The seasonal targets consist of season onset, defined as the first MMWR week where wILI is at or above baseline and remains above for three consecutive weeks, season peak week, defined as the MMWR week of maximum wILI, and season peak percentage, defined as the maximum wILI value for the season. The short-term targets consist of forecasts for wILI values 1, 2, 3, and 4 weeks ahead of the most recently published data. With the two-week reporting delay in the publication of ILINet, these forecasts are for the level of wILI occurring 1 week prior to the week the forecast is made, the current week, and the two weeks after the forecast is made. *(Could include comparison manuscript diagram here)* Forecasts are created for all targets for the US as a whole and for each of the 10 HHS Regions.

For all targets, forecasts consist of probability distributions within bins of possible values for the target. For season onset and peak week, forecast bins consist of individual weeks within the influenza season, with an addition bin for onset week corresponding to a forecast of no onset. For short-term targets and peak intensity, forecast bins consist of levels of observed wILI rounded to the nearest 0.1\% up to 13\%, which is the level of resolution publicly for ILINet reported by the CDC. Formally, the bins are defined as $[0.00, 0.05),\ [0.05, 0.15),\ \dots,\ [12.85, 12.95),\ [12.95, 100]$. 

\subsection{Forecast Evaluation}
Submitted forecasts were evaluated using the modified log score used by the CDC in their forecasting challenge, which provides a simultaneous measure of forecast accuracy and precision. The log score for a probabalistic forecast $m$ is defined as $\log f_m(z^*|\bf{x})$, where $f_m(z|\bf{x})$ is the predicted density function from model $m$ for some target $Z$, conditional on some data $\bf{x}$ and $z^*$ is the observed value of the target $Z$. 

While a true log score only evaluates the probability assigned to the exact observed value $z^*$, the CDC uses a modified log score that classifies additional values as ``accurate``. For predictions of season onset and peak week, probabilities assigned to the week before and after the observed week are included as correct, so the modified log score becomes $\log \int_{z^* -1}^{z^* + 1} f_m(z|{\bf{x}})dz$. For season peak percentage and the short-term forecasts, probabilities assigned to wILI values within 0.5\% of the observed values are included as correct, so the modified log score becomes $\log \int_{z^* -.5}^{z^* + .5} f_m(z|{\bf{x}})dz$. We refer to these modified log scores as simply log scores hereafter.

Individual log scores can be averaged across different combinations of forecast regions, target, weeks, or seasons. Formally, each model $m$ has a large number of region-, target-, season-, and week-specific log scores, and we represent a specific scalar log score as $\log f_{m,r,t,s,w}(z^*|\bf{x})$. These individual log scores can be averaged across combinations of regions, targets, seasons, and weeks to compare model performance.

\subsection{Component models}
To provide training data for the ensemble, five teams submitted between 1 and 9 models each, for a total of 22 component models. *(NOTE: Could refer to comparison manuscript here if it's out)* Teams submitted out-of-sample forecasts for the 2010/2011 through 2016/2017 influenza seasons. Teams constructed their forecasts in a prospective fashioon, using only data that were available at the time of the forecast. For some data sources (i.e. wILI prior to the 2014/2015 influenza season), data as they were published at the time were not available. In such cases, teams were still allowed to use those data sources while making efforts to only use data available at the time forecasts would have been made.

For each influenza season, teams submitted weekly forecasts from epidemic week 40 (EW40) of the first year through EW20 of the following year, using standard CDC definitions for epidemic week (citation). If a season contained EW53, forecasts were submitted for that week as well. In total, teams submitted 233 individual forecast files representing forecasts across the seven influenza seasons. Once submitted, the forecast files were not updated except in four instances were explicit programming bugs had resulted in numerical issues in the forecast. Teams were explicitly discouraged from re-tuning or adjusting their models for different prior seasons to avoid issues with over-fitting.

*Should this go in the results?*
Teams utilized a variety of methods and modeling approaches in the construction of their submissions. Seven of the models used a compartmental structure in their models (i.e. Susceptible-Infectious-Recovered) to model the disease transmission process in some way, while other models used more statistical approaches to directly model the observed wILI curve. Six of the models explicitly incorporate additional data sources beyond previous wILI data, including weather data and Google search data.  

\subsection{Ensemble Construction}
We considered 


\bibliographystyle{unsrt}
\bibliography{../flusightnetwork.bib}

\end{document}