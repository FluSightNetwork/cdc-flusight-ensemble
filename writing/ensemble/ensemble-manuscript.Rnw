\documentclass{article}

\title{A Collaborative Ensemble Approach to Real-Time Influenza Forecasting in the U.S.: Results from the 2017/2018 Season}

\author{Nicholas G Reich$^1$, Logan Brooks$^2$, Sasikiran Kandula$^3$, Craig McGowan$^4$,\\Evan Moore$^1$, Dave Osthus$^5$, Evan Ray$^6$, Abhinav Tushar$^1$, Teresa Yamana$^3$, \\Matthew Biggerstaff$^4$, Michael A Johansson$^7$, Roni Rosenfeld$^2$, Jeffrey Shaman$^3$}

\date{%
    $^1$University of Massachusetts-Amherst, Amherst, USA\\%
    $^2$Carnegie Mellon University, Pittsburgh, USA\\%
    $^3$Columbia University, New York, USA\\
    $^4$Influenza Division, Centers for Disease Control and Prevention, Atlanta, USA\\
    $^5$Los Alamos National Laboratory, Los Alamos, USA\\
    $^6$Mount Holyoke College, South Hadley, USA\\
    $^7$Division of Vector-Borne Diseases, Centers for Disease Control and Prevention, Atlanta, USA\\
}
\usepackage[letterpaper, margin=1in]{geometry} % margin
\usepackage{lineno}% add line numbers
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{parskip}        % for spacing after paragraphs http://ctan.org/pkg/parskip
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath, amsfonts}
\usepackage{setspace}
\linenumbers % line numbers
\onehalfspacing



% For computer modern sans serif
\usepackage[T1]{fontenc}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif


\begin{document}

%\SweaveOpts{concordance=TRUE}

\maketitle

\tableofcontents

<<echo=FALSE, warning=FALSE, message=FALSE>>=
knitr::opts_chunk$set(
  echo = FALSE, cache = FALSE, cache.path = './cache', message = FALSE, warning = FALSE
)
library(dplyr)
library(readr)
library(ggplot2)
library(MMWRweek)
library(xtable)
library(cdcfluview)
library(gridExtra)

theme_set(theme_minimal())
specify_decimal <- function(x, k=0) trimws(format(round(x, k), nsmall=k))
@


<<read-weights>>=
target_type_weights <- read.csv("../../weights/target-type-based-weights.csv") %>%
    filter(target == "1 wk ahead" | target=="Season onset") %>%
    mutate(component_model_id = reorder(component_model_id, weight))
@


<<generate-scores>>=
scores <- read_csv("../../scores/scores-with-20172018.csv")
models <- read_csv("../../model-forecasts/component-models/model-id-map.csv")
targets <- read_csv("../../scores/target-multivals.csv")

complete_models <- c(models$`model-id`[models$complete=="true"])

## define column with scores of interest
SCORE_COL <- quo(`Multi bin score`)

## Create data.frame of boundary weeks of scores to keep for each target/season
all_target_bounds <- read_csv("data/all-target-bounds.csv")

## Remove scores that fall outside of evaluation period for a given target/season
scores_trimmed <- scores %>% 
    dplyr::left_join(all_target_bounds, by = c("Season", "Target", "Location")) %>%
    dplyr::filter(`Model Week` >= start_week_seq, `Model Week` <= end_week_seq)

## truncate lowest possible scores to -10, define target-type variable
scores_adj <- scores_trimmed %>%
    filter(Model != "UTAustin-edm") %>%
    ## if NA, NaN or <-10, set score to -10
    mutate(score_adj = dplyr::if_else(is.nan(!!SCORE_COL) | is.na(!!SCORE_COL) , 
        -10, 
        !!SCORE_COL),
        target_type = dplyr::if_else(Target %in% c("Season onset", "Season peak week", "Season peak percentage"),
            "seasonal", "k-week-ahead")) %>%
    mutate(
        score_adj = dplyr::if_else(score_adj < -10 , -10, score_adj),
        Location = factor(Location, levels=c("US National", paste("HHS Region", 1:10))),
        Model = reorder(Model, score_adj),
        #Season = reorder(Season, score_adj),
        Location = reorder(Location, score_adj)
        ) 

training_scores_by_model <- scores_adj %>%
    filter(Season != "2017/2018") %>%
    group_by(Model) %>%
    summarize(
        avg_logscore = mean(score_adj)
    ) %>%
    ungroup()

training_scores_by_model_season <- scores_adj %>%
    filter(Season != "2017/2018") %>%
    group_by(Model, Season) %>%
    summarize(
        avg_logscore = mean(score_adj)
    ) %>%
    ungroup()

test_scores_by_model <- scores_adj %>%
    filter(Season == "2017/2018") %>%
    group_by(Model, Season) %>%
    summarize(
        avg_logscore = mean(score_adj)
    ) %>%
    ungroup()


test_scores_by_model_tt <- scores_adj %>%
    filter(Season == "2017/2018") %>%
    group_by(Model, target_type) %>%
    summarize(
        avg_logscore = mean(score_adj),
        avg_score = exp(avg_logscore)
    ) %>%
    ungroup()

@


\clearpage

\begin{abstract}
TBD.
\end{abstract}

\begin{itemize}
\item XX Table 1: model table with ensembles
\item Figure 1: Flu Forecasting/Ensemble Overview
\item XX Figure 2: Seasonal average Results figure for all models, all seasons
\item XX Figure 3: Model weights
\item Figure 4: Results for 2017/2018: by region or target?

\item Suppl Figure: CV and real-time results for ensemble models, sorted by complexity
\end{itemize}

\clearpage

\section{Introduction}

Outline
\begin{itemize}
    \item Ensembles are good - weather and inf. disease.
    \item Simple ensembles ok, evidence weighted ensembles better
    \item CDC forecasting flu for a while - history of challenge means number of teams with models to potentially combine
    \item Formation of FluSight Network
    \item Overview of goals, methods, etc.
\end{itemize}

Ensemble models, or models that fuse together predictions from multiple different models, have long been seen as a valuable method for improving predictions over any single model. This "wisdom of the crowd" approach (where the "crowd" can be thought of as a throng of models) has both theoretical and practical advantages. First, it allows for an ensemble forecast to incorporate signals from different data sources and models that may highlight different features of a system. Second, combining signals from models with different biases may allow those biases to offset and result in an ensemble that is more accurate than the individual component models. Weather and climate models have utilized ensemble systems for these very purposes, and recent work has extended ensemble forecasting to forecasts of infectious diseases, including influenza , dengue fever, and Ebola hemorrhagic fever %\cite{Viboud2018}.

Since the 2013/2014 influenza season, the Centers for Disease Control and Prevention (CDC) has run an annual prospective influenza forecasting competition, known as the FluSight challenge, in collaboration with outside researchers. Participating teams submit probabalistic forecasts for a variety of influenza targets weekly from early November through mid May. During the 2015/2016 and 2016/2017 challenges, analysts at the CDC built a simple ensemble model by taking the arithmetic mean of submitted models. This model was one of the top performing models each season (cite McGowan et al when accepted).

Given the success of a simple ensemble that incorporated no information about the relative performace of component models, an ensemble taking component model performance into account has the potential for further improvments. In March 2017 the FluSight Network, a collaborative group of influenza forecasters who have worked with the CDC in the past, was established to facilitate the pooling of resources to develop an ensemble that could incorporate past performance of models. This group worked throughout 2017 to create a set of guidelines and an experimental design that would enable submission of a publicly available, multi-team, real-time submission of an ensemble model with validated and performance-based weights for each model. 

This paper describes the development of 


\section{Results}

\subsection{Summary of component models}

\subsubsection*{General description of models included, targets, regions, etc. }
We assembled 21 component models to be evaluated for inclusion in our ensemble models (Table \ref{tab:models}).
These models encompassed a variety of different modeling philosophies, including Bayesian hierarchical models, mechanistic models of infectious disease transmission, statistical learning methodologies, and classical statistical models for time-series data.
As a pre-requisite for inclusion in the ensemble, every model submitted seven seasons of out-of-sample forecasts.
As the goal was to submit our ensemble forecast to the CDC FluSight forecasting challenge, we adhered to guidelines and formats set forth by the challenge in determining forecast format.
A season consists of forecast files generated weekly for 33 or 34 weeks, starting with epidemic week 40 (EW40) of one calendar year and ending with EW20 of the following year.
A model's weekly submission contained probabilistic and point forecasts for seven targets in each of 11 regions in the U.S. (national-level plus the 10 Health and Human Services (HHS) regions).
Targets can be thought of as being of two types: four of the targets are weekly incidence targets that change every week and the three remaining targets are the seasonal targets that do not change over the course of the season (see Methods).

\input{./static-content/model-table.tex}

\subsubsection*{Describe observed variation in performance across training seasons}

Individual component model forecast performance in the seven training seasons (2010/2011 - 2016/2017) varied widely across region, season, and target.
A detailed comparative analysis of component model forecast performance can be found elsewhere,\cite{FluSightNetwork2018} however we summarize a few key insights from model performance here.
Using the average log-score as a measure of forecast performance (again, following CDC challenge convention), we observed that around 2/3 of the component models reliably out-performed a seasonal average model on all targets. 
[[Explain metric here!]]
Over 50\% of the component models out-performed the seasonal baseline model in forecasting 1- and 2-week ahead incidence as well as season peak percentage and season peak week.

\subsubsection*{Quantitative summaries of how much forecasting data collected and how stored/scored}

Each component model's weekly forecasts were submitted in real-time to a central data repository throughout the 2017/2018 influenza season.\cite{FSNGithub2018}
The season began with submissions for epidemic week 43, 2017 (EW43-2017) on Monday, November 6, 2017 and ended with submissions for EW18-2018 on Monday, May 18, 2018.
In all, the seven seasons of training data consisted of about 95GMB of data per model and about 2GB of forecast data for all models combined.
The real-time forecasts for the 2017/2018 season added about 300MB of data.


\subsection{Target-type weights selected based on cross-validation}

\subsubsection*{Ensembles in general showed more accuracy than component models}
Prior to any systematic evaluation of component model performance and prior to the 2017/2018 season, we pre-specified five ensemble models and evaluated their performance in the seven training seaons.
The five pre-specified ensemble models all were built using weighted model averages of the component models.
Each ensemble was defined by the number of weights estimated. 
The simplest ensemble took a simple equal-weight average of all models regardless of past performance.
The most complex estimated weights separately for each model, target, and location, for a total of XX weights (see Methods).
Across the seven training seasons, all ensemble models but the simple average showed higher overall average forecast score than any single component model (Figure \ref{fig:season-average-results}). 

<<season-average-results, fig.height=5, fig.cap="Average forecast score, aggregated across targets, regions, and weeks, plotted separately for each model and season. Models are sorted from lowest scores (left) to highest scores (right). Higher scores indicate better performance. Dots show average scores across all targets, regions, and weeks within a given season. The `x' marks the geometric mean of the seven seasons. The {\\tt ReichLab-KDE} model (italicized red font) is considered the historical baseline model.">>=
pal=c(RColorBrewer::brewer.pal(7, "Dark2"), "#000000", "#000000")

ordered_models <- rbind(
    training_scores_by_model_season#,
    #test_scores_by_model 
    ) %>%
    mutate(Model = reorder(factor(Model), avg_logscore)) %>% 
    .$Model %>% levels()

tmp <- rbind(
    training_scores_by_model_season,
    cbind(training_scores_by_model, Season = "training average"),
    mutate(test_scores_by_model, Season = "2017/2018 real-time")) %>%
    mutate(Model = factor(Model, levels=ordered_models, ordered=TRUE))

ggplot(tmp, aes(x=Model, y=exp(avg_logscore))) +
    geom_point(aes(color=Season, shape=Season, alpha=Season, size=Season)) + 
    #scale_color_brewer(palette="Dark2") +
    scale_color_manual(name="", values=pal) +
    scale_shape_manual(name="", values=c(rep(16, 8), 120)) +
    scale_alpha_manual(name="", values=c(rep(.5, 7), 1, 1)) +
    scale_size_manual(name="", values=c(rep(2, 8), 3)) +
    ylab("average forecast score") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5, 
        #family="Courier",
        face=ifelse(
            levels(training_scores_by_model_season$Model) == "ReichLab-KDE", 
            "bold",
            "plain"
        ),
        color = ifelse(
            grepl("FSNetwork", levels(training_scores_by_model_season$Model)), 
            "red",
            "black"
        )
        ))

# ggplot(training_scores_by_model_season, aes(x=Model, y=exp(avg_logscore))) +
#     geom_point(alpha=.5, aes(color=Season)) + 
#     geom_point(data=training_scores_by_model, aes(color="average"), shape="x", size=1, stroke=5)+
#     geom_point(data=test_scores_by_model_season, aes(color=Season))+
#     #scale_color_brewer(palette="Dark2") +
#     scale_color_manual(values=pal) +
#     ylab("average forecast score") +
#     theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5, 
#         #family="Courier",
#         face=ifelse(
#             levels(training_scores_by_model_season$Model) == "ReichLab-KDE", 
#             "bold",
#             "plain"
#         ),
#         color = ifelse(
#             grepl("FSNetwork", levels(training_scores_by_model_season$Model)), 
#             "red",
#             "black"
#         )
#         )) 

@


\subsubsection*{Target-type weights achived highest cross-validated scores}
We chose the target-type weights model as the model that would be submitted in real-time to the CDC during the 2017/2018 season, based on leave-one-season-out cross-validated performance on the seven seasons of training data.
The FluSightNetwork's target-type-weights ({\tt FSNetwork-TTW}) model estimated 42 weights, one for each model and target-type combination.
The {\tt FSNetwork-TTW} model outperformed all other ensemble models in the training phase by a slim margin, achieving an average forecast score of XX, compared with XX, XX and XX.


\subsubsection*{Weights distributed across models from all teams}
Using performance of all component models across the seven training seasons, we estimated weights for the four ensemble models with varying weights.
The {\tt FSNetwork-TTW} model assigned non-negligible weight to XX models for week-ahead targets and XX models for seasonal targets (Figure \ref{fig:ttweights}).
For week-ahead targets, the non-zero weights varied from 0.XX for the XX model to 0.XX for the XX model.
For seasonal targets, the non-zero weights varied from 0.XX for the XX model to 0.XX for the XX model, with XX models being assigned above 0.10 weight.
All four teams had at least one model with non-zero weight in the submitted model. 

<<ttweights, fig.height=3, fig.cap="Model weights for the FluSight Network Target-Type Weights ({\tt FSNetwork-TTW}) model. Weights were estimated using cross-validated forecast performance in the 2010/2011 through the 2016/2017 seasons.">>=
ttweights_2017 <- filter(target_type_weights, season=="2017/2018") %>%
    mutate(tt = ifelse(target=="1 wk ahead", "week-ahead", "seasonal"))
ggplot(ttweights_2017, 
    aes(y=tt, fill=weight, x=component_model_id)) + 
    geom_tile() + ylab(NULL) + xlab(NULL) +
    geom_text(aes(label=round(weight, 2)), size=2) +
    scale_fill_gradient(low = "white", high="dodgerblue4", limits=c(0,1)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
@


\subsection{Summary of ensemble real-time performance in 2017/2018 season}

\subsubsection*{Description of general discrepancy between 2017/2018 and other seasons}

The 2017/2018 influenza season in the U.S. showed dynamics that were unlike that of any season in the past 15 years.
As measured by wILI percent at the national level, the 2017/2018 season was on par with the other two highest peaks on record: the 2003/2004 season and the 2009 H1N1 outbreak in June.
In some regions, for example HHS Region 2 (New York and New Jersey) and HHS Region 4 (southeastern states), the highest reported wILI percent for the 2017/2018 season was more than two percentage points higher than had ever been observed before, an increase of over 20\% above previously observed peaks.
Because all forecasting models rely, to some extent, on future trends mimicing observed patterns in the past, the anomalous dynamics in 2017/2018 posed a challenging ``test season'' for all models, including the new ensembles.

\subsubsection*{Specific evaluation of TTW model's performance compared to other component models}

The chosen ensemble model submitted to the CDC in real-time throughout the 2017/2018 season, {\tt FSNetwork-TTW}, showed continued accurate performance.
Averaging across all targets and regions, the {\tt FSNetwork-TTW} model showed XX forecast score.
The {\tt ReichLab-SARIMA2} and {\tt LANL-DBM} model both showed similar overall scores, of XX and XX.
[[Make same comparison by target-types and by region?]]

<<realtime-tt-scores, fig.height=3, fig.cap="Average forecast score in 2017/2018 by model target-type. The text within the grid shows the score itself. The white midpoint of the color scale is set to be the target-specific average of the historical baseline model, {\\tt ReichLab-KDE}, with darker blue colors representing models that have better scores than the baseline and darker red scores representing models that have worse scores than the baseline. The models are sorted in descending order from most accurate (right) to least accurate (left).">>=
test_scores_by_model_tt %>%
    group_by(target_type) %>%
    mutate(
        baseline_score = avg_logscore[Model=="ReichLab-KDE"]
    ) %>%
    ungroup() %>%
    mutate(
        baseline_skill = exp(baseline_score),
        pct_diff_baseline_skill = (exp(avg_logscore) - baseline_skill)/baseline_skill
    ) %>%
ggplot(
    aes(y=target_type, x=Model, fill=pct_diff_baseline_skill*100)) + 
    geom_tile() + ylab(NULL) + xlab(NULL) +
    #facet_grid(~target_type) +
    geom_text(aes(label=specify_decimal(avg_score, 2)), size=2) +
    scale_fill_gradient2(name="% change \nfrom baseline") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5))
@


\subsubsection*{Comparison of CV ordering vs. 2017/2018 ordering: Comparison of ensemble models to non-ensemble models in 2017/2018 and comparison within ensembles}

The model accuracy patterns observed in the cross-validation phase was largely preserved in the real-time phase.
[[add model ranks to table (avg for CV years, single for 2017/2018)]]
In the cross-validation phase, ensemble models had slightly higher average performance than all component models, and [[ranked as the top model in each season]].
This pattern was broken in the 2017/2018 season, with [[two component models]] showing better performance than the chosen ensemble in across all locations and targets.[[although does this break down into better performance in all locations/targets or just some?]]



\subsubsection*{Summary of performance (reduced accuracy) in anomalous 2017/2018 season}

Due to the anomalous dynamics of the 2017/2018 season, all models suffered from reduced forecast accuracy.
Summarize overall accuracy reductions
Summarize target-specific accuracy reductions
Summarize region-specific accuracy reductions

\subsubsection*{post-hoc analysis of other possible weighting schemes (discussion of NN, "follow-the-leader", ...)}

Post-hoc analyses of alternative ensemble weighting schemes that followed a more in-season adaptive  approach to assigning weights to specific models showed that such approaches would have provided some marginal benefit over the chosen approaches.


\section{Discussion}

Outline
\begin{itemize}
  \item Overall, the ensemble methods outperform component models. There are some specific situations where the component models do better, but that is to be expected when testing a lot of situations.
  \item Comparisons to unweighted average
  \item Comparisons to component models
  \item Strengths and weaknesses of ensemble approach
  \item Practicality and real-world impacts
\end{itemize}


\section{Methods}

\subsection{Influenza Data}
Forecasting targets for the CDC FluSight challenge are based on the US Outpatient Influenza-like Illness Surveillance Network (ILINet). 
ILINet is a syndromic surveillance system that measures the weekly percentage of outpatient visits due to influenza-like illness (ILI) from a network of more the 2,800 providers, and publishes a weighted estimate of ILI (wILI) based on state populations. 
Estimates of wILI are reported weekly by the CDC's Influenza Division for the United States as a whole as well as for each of the 10 Health and Human Services (HHS) regions. 
Reporting of 'current' wILI is typically delayed by approximately two weeks as data are collected and processed, and each weekly publication can also include revisions of prior reported values if new data become available. 
For the U.S. and each HHS Region, CDC publishes an annual baseline level of ILI activity based on off-season ILI levels. 

\subsection{Forecast Targets and Structure}
Forecasts for the CDC FluSight challenge consist of seven targets, three seasonal targets and four short-term targets. The seasonal targets consist of season onset, defined as the first MMWR week where wILI is at or above baseline and remains above for three consecutive weeks, season peak week, defined as the MMWR week of maximum wILI, and season peak percentage, defined as the maximum wILI value for the season. The short-term targets consist of forecasts for wILI values 1, 2, 3, and 4 weeks ahead of the most recently published data. With the two-week reporting delay in the publication of ILINet, these forecasts are for the level of wILI occurring 1 week prior to the week the forecast is made, the current week, and the two weeks after the forecast is made. *(Could include comparison manuscript diagram here)* Forecasts are created for all targets for the US as a whole and for each of the 10 HHS Regions.

For all targets, forecasts consist of probability distributions within bins of possible values for the target. For season onset and peak week, forecast bins consist of individual weeks within the influenza season, with an addition bin for onset week corresponding to a forecast of no onset. For short-term targets and peak intensity, forecast bins consist of levels of observed wILI rounded to the nearest 0.1\% up to 13\%, which is the level of resolution publicly for ILINet reported by the CDC. Formally, the bins are defined as $[0.00, 0.05),\ [0.05, 0.15),\ \dots,\ [12.85, 12.95),\ [12.95, 100]$. 

\subsection{Forecast Evaluation}
Submitted forecasts were evaluated using the modified log score used by the CDC in their forecasting challenge, which provides a simultaneous measure of forecast accuracy and precision. The log score for a probabalistic forecast $m$ is defined as $\log f_m(z^*|\bf{x})$, where $f_m(z|\bf{x})$ is the predicted density function from model $m$ for some target $Z$, conditional on some data $\bf{x}$ and $z^*$ is the observed value of the target $Z$. 

While a true log score only evaluates the probability assigned to the exact observed value $z^*$, the CDC uses a modified log score that classifies additional values as ``accurate``. For predictions of season onset and peak week, probabilities assigned to the week before and after the observed week are included as correct, so the modified log score becomes $\log \int_{z^* -1}^{z^* + 1} f_m(z|{\bf{x}})dz$. For season peak percentage and the short-term forecasts, probabilities assigned to wILI values within 0.5\% of the observed values are included as correct, so the modified log score becomes $\log \int_{z^* -.5}^{z^* + .5} f_m(z|{\bf{x}})dz$. We refer to these modified log scores as simply log scores hereafter.

Individual log scores can be averaged across different combinations of forecast regions, target, weeks, or seasons. Formally, each model $m$ has a large number of region-, target-, season-, and week-specific log scores, and we represent a specific scalar log score as $\log f_{m,r,t,s,w}(z^*|\bf{x})$. These individual log scores can be averaged across combinations of regions, targets, seasons, and weeks to compare model performance.

\subsection{Component models}
To provide training data for the ensemble, four teams submitted between 1 and 9 models each, for a total of 21 component models. *(NOTE: Could refer to comparison manuscript here if it's out)* Teams submitted out-of-sample forecasts for the 2010/2011 through 2016/2017 influenza seasons. Teams constructed their forecasts in a prospective fashioon, using only data that were available at the time of the forecast. For some data sources (i.e. wILI prior to the 2014/2015 influenza season), data as they were published at the time were not available. In such cases, teams were still allowed to use those data sources while making efforts to only use data available at the time forecasts would have been made.

For each influenza season, teams submitted weekly forecasts from epidemic week 40 (EW40) of the first year through EW20 of the following year, using standard CDC definitions for epidemic week (citation). If a season contained EW53, forecasts were submitted for that week as well. In total, teams submitted 233 individual forecast files representing forecasts across the seven influenza seasons. Once submitted, the forecast files were not updated except in four instances were explicit programming bugs had resulted in numerical issues in the forecast. Teams were explicitly discouraged from re-tuning or adjusting their models for different prior seasons to avoid issues with over-fitting.

*Should this go in the results?*
Teams utilized a variety of methods and modeling approaches in the construction of their submissions. Seven of the models used a compartmental structure in their models (i.e. Susceptible-Infectious-Recovered) to model the disease transmission process in some way, while other models used more statistical approaches to directly model the observed wILI curve. Six of the models explicitly incorporate additional data sources beyond previous wILI data, including weather data and Google search data.  

\subsection{Ensemble Construction}
We considered 


\bibliographystyle{unsrt}
\bibliography{../flusightnetwork.bib}

\end{document}