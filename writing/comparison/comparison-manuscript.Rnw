\documentclass{article}

\title{Collaborative Multi-Season Evaluation of Annual Influenza Forecasting Models in the U.S.}

\author{Logan Brooks, Spencer Fox, Craig McGowan, Sasikiran Kandula, \\ Dave Osthus, Evan Ray, Nicholas G Reich, Roni Rosenfeld, Jeffrey Shaman, \\Abhinav Tushar, Teresa Yamana [authorship list to be finalized]}

\usepackage[letterpaper, margin=1in]{geometry} % margin
\usepackage{lineno}% add line numbers
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{parskip}        % for spacing after paragraphs http://ctan.org/pkg/parskip
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath, amsfonts}
\usepackage{setspace}
\linenumbers % line numbers
\onehalfspacing



% For computer modern sans serif
\usepackage[T1]{fontenc}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif


\begin{document}

\maketitle

\tableofcontents

<<echo=FALSE, warning=FALSE, message=FALSE>>=
knitr::opts_chunk$set(
  echo = FALSE, cache = FALSE, cache.path = './cache', message = FALSE, warning = FALSE
)
library(dplyr)
library(readr)
library(ggplot2)
library(MMWRweek)
library(xtable)
library(cdcfluview)
library(fiftystater)
library(mapproj)
library(gridExtra)

theme_set(theme_minimal())
specify_decimal <- function(x, k=0) trimws(format(round(x, k), nsmall=k))
@


\section{Introduction}

In recent years, the quantity of research on forecasting infectious diseases has increased XX fold. 
This increased interest has been fueled in part by the promise of `big data', that near real-time data streams of large-scale population behavior \cite{Molodecky2017} to microscopic changes in a pathogen \cite{Du2017} could lead to measurable improvements in how disease transmission is measured, forecasted, and prevented \cite{Bansal2016}. 
With the spectre of a global pandemic looming, improving infectious disease forecasting continues to be a central priority of global health preparedness efforts.\cite{Myers2000,WHO2016}

Forecasts of infectious disease transmission can inform public health response to outbreaks. 
Accurate forecasts of the timing and spatial spread of seasonal outbreaks of diseases such as influenza or dengue fever can provide valuable information about where public health interventions can be targeted.
Decisions about hospital staffing, resource allocation, and the timing of public health communication campaigns could be assisted by forecasts. 
Implementation of interventions designed to disrupt disease transmissions, such as vector control measures or mandatory infection prevention protocols at hospitals or helath clinics, could be targeted based on forecasted incidence.

Public health officials are still learning how to best integrate forecasts into real-time decision making.
Close collaboration between public health policy-makers and quantitative modelers is necessary to ensure the forecasts have maximum impact and are appropriately communicated to the public and the broader public health community. 
Understanding what targets should be forecasted for maximum public health impact is hard to assess without real-time implementation and testing.


Starting in the 2013-2014 influenza season, the U.S. Centers for Disease Control and Prevention (CDC) has run the "Forecast the Influenza Season Collaborative Challenge" (a.k.a. FluSight) each influenza season, soliciting weekly forecasts for specific influenza season metrics from teams across the world.
These forecasts are displayed together on a website during the season and are evaluated for accuracy after the season is over.\cite{PhiResearchLab} 
This effort has galvanized a community of scientists interested in forecasting, creating an organic testbed for improving both our technical understanding of how different forecast models perform but also how to integrate these models into decsision-making.

Building on the structure of the FluSight challenges (and those of other collaborative forecasting efforts\cite{Viboud}), a subset of participants founded a consortium to facilitate direct comparison and fusion of modeling approaches. 
In this paper, we provide a detailed analysis of the performance of 22 different models from 5 different teams over the course of seven influenza seasons.
Drawing on the different expertise of the five teams allows us to make fine-grained and standardized comparisons of distinct modeling approaches that using different data sources.
Additionally, it allows us to identify gaps and continued challenges that should be addressed in future modeling efforts. 



% Infectious disease modeling has proven to be fertile ground for statisticians, mathematicians, and quantitative modelers for over a century. 
% Yet there is not a consensus on a single best modeling approach or method for forecasting the dynamic patterns of infectious disease outbreaks, in both endemic and emergent settings. 
% Mechanistic models consider the biological underpinnings of disease transmission, and are in practice are typically implemented as variations on the Susceptible-Infectious-Recovered (SIR) model. 
% Phenomenological models largely ignore the biological underpinnings and theory of disease transmission and focus instead on using data-driven, empirical and statistical approahces to make the best forecasts possible of a given dataset, or phenomenon. 
% Both approaches are commonly used and both have advantages and disadvantages in different settings.   


\section{Methods}

\subsection{FluSight Challenge Overview}
 
Detailed methodology and results from previous FluSight challenges have been published\cite{Biggerstaff2016}, but we summarize the key features of the challenge here.

The FluSight challenge focuses on forecasts of the weighted percentage of doctor's office visits for influenza-like-illness (wILI) in a particular region. 
This is a standard measure of seasonal flu activity, for which public data is available for the US back to the 1997/1998 influenza season. 
During each influenza season, this data is updated each week by the CDC (Figure \ref{fig:timezero-schematic}). When the most recent data is released, the prior weeks' reported wILI data may also be revised. 
The unrevised data, available at a particular moment in time, is available via the DELPHI real-time epidemiological data API beginning in the 2013/2014 season.\cite{DELPHI} 
This API enables researchers to ``turn back the clock'' to a particular moment in time and use the data available at that time. This enables more accurate assessment of how models would have performed in real-time. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{static-figures/timezero-sketch.pdf}
\caption{(A) Raw data weighted influenza-like illness data downloaded from the CDC website. The y-axis shows the weighted percentage of doctor's office visits for influenza-like illness for each week between September 2010 through July 2017, which is the time period for which the models presented in this paper made seasonal forecasts. (B) A diagram showing the anatomy of a single forecast. The seven forecasting targets are illustrated with a point estimate (dot) and interval (uncertainty bars). The five the targets on the wILI scale are shown with uncertainty bars spanning the vertical wILI axis, while the two targets for a time-of-year are illustrated with horizontal uncertainty bars along the temporal axis. The onset is defined relative to a region-specific baseline wILI percentage defined by the CDC. Arrows illustrate the timeline for a typical forecast for the CDC FluSight challenge, assuming that forecasts are generated or submitted to the CDC using the most recent reported data. This data includes the first reported observations of wILI\% from two weeks prior. Therefore, 1 and 2 week-ahead forecasts are considered nowcasts, i.e. at or before the current time. Similarly, 3 and 4 week-ahead forecasts are considered proper forecasts, or estimates about events in the future.}
\label{fig:intro-schematic}
\end{center}
\end{figure}


<<eval=FALSE>>=
## code for plot in Figure 1
regionflu <- get_flu_data("hhs", sub_region=1:10, data_source="ilinet", years=1997:2017)
usflu <- get_flu_data("national", sub_region=NA, data_source="ilinet", years=1997:2017)

## make AGE cols in usflu integer data type
cols <- grepl('^AGE', colnames(regionflu))
regionflu[,cols] <- sapply(regionflu[,cols], as.integer)
cols <- grepl('^AGE', colnames(usflu))
usflu[,cols] <- sapply(usflu[,cols], as.integer)


fludata <- bind_rows(regionflu, usflu)

fludata <- dplyr::transmute(fludata,
    region.type = `REGION TYPE`,
    region = REGION,
    year = YEAR,
    week = WEEK,
    caldate = as.POSIXct(MMWRweek2Date(YEAR, WEEK)),
    weighted_ili = as.numeric(`% WEIGHTED ILI`))

fludata$region[fludata$region == "X"] <- "National"
fludata$region <- factor(fludata$region, 
    levels=c("National", paste("Region", 1:10)),
    labels=c("National", paste("HHS Region", 1:10)))

fludata %>%
    filter(
        region %in% c("National", paste("HHS Region", c(1,5,6,7,9))),
        caldate > as.POSIXct(as.Date("2010-09-01")),
        caldate < as.POSIXct(as.Date("2017-08-01"))) %>%
    ggplot(aes(x=caldate, y=weighted_ili, color=region)) + 
    xlab(NULL) + ylab("weighted ILI (%)") +
    geom_line() +
    scale_color_brewer(palette="Dark2")
@


The FluSight challenges have defined seven forecasting targets of particular public health relevance. Three of these targets are fixed scalar values for a particular season: onset week, peak week, and peak intensity (i.e. the maximum observed wILI percentage). The remaining four targets are the observed wILI percentages in each of the subsequent four weeks (Figure \ref{fig:intro-schematic}). 

The FluSight challenges have also required that all forecast submissions and follow a particular format. A single submission file (a comma-separated text file) contains the forecast made for a particular epidemic week (EW) of a season. Standard CDC definitions of epidemic week are used. Each file contains binned predictive distributions for seven specific targets across the 10 HHS regions of the US plus the national level. Each file contains over 8000 rows and typically is about 400KB in size.

To be included in the model comparison presented here, previous participants in the CDC FluSight challenge were invited to provide out-of-sample forecasts for the 2010/2011 through 2016/2017 seasons. For each model, this involved creating 233 separate forecast submission files, one for each of the weeks in the seven training seasons.
Each forecast file represented a single submission file, as would be submitted to the CDC challenge. 
Each team created their submitted forecasts in a prospective, out-of-sample fashion, i.e. fitting or training the model only on data available before the time of the forecast (see Figure \ref{fig:intro-schematics}). 

\subsection{Summary of Models}

Five teams each submitted between 1 and 9 separate models for evaluation (Table \ref{tab:model-list}). 
A wide range of methodological approaches and modeling paradigms are included in the set of forecast models.
For example, seven of the models utilize a compartmental structure (e.g. Susceptible-Infectious-Recovered), a model framework that directly encodes both the transmission and the susceptible-limiting dynamics of infectious disease outbreaks.
Other less directly mechanistic models use statistical approaches to model the outbreak phenomenon directly by incorporating recent incidence and seasonal trends.
[[Six]] models directly incorporate external data (i.e. not just the wILI measurements from the CDC ILINet dataset), including historical humidity data and Google search data.
Two models stand out as being clear na\"ive baseline models, that never change based on recent data. 
The {\tt Delphi-Uniform} model always provides a forecast that assigns equal probability to all possible outcomes. 
The {\tt ReichLab-KDE} model yields predictive distributions based entirely on data from other seasons using kernel density estimation (KDE) for seasonal targets and a generalized additive model with cyclic penalized splines for weekly incidence.
Throughout the manuscript when we refer to the `historical baseline` model we mean the {\tt ReichLab-KDE} model.


\input{./static-figures/model-table.tex}

\subsection{Metrics Used for Evaluation and Comparison}

Forecasts have historically been evaluated by the CDC using two metrics, the log-score and the mean absolute error. These two metrics capture different desirable features of performance. The log-score enables evaluation of both the precision and accuracy of a forecast, using the predicted density function.\cite{Gneiting2007} The absolute error provides an interpretable summary of the amount of error the point estimates had on average.\cite{Reich2016} 

We used a modified form of the log-score to evaluate forecasts, in line with the evaluation performed by the CDC. The log-score is defined as $\log f(\hat z|\bf{x})$ where $f(z|\bf{x})$ is a predictive density function for some target $Z$, conditional on some data $\bf{x}$ and $\hat z$ is the observed value of the target $Z$. 
In practice, each model $m$ has a set of log scores associated with it are region-, target-, season-, and week- specific.
For example, one specific scalar score value is represented as $\log f_{m,r,t,s,w}(\hat z|\bf{x})$. 
We evaluated model performance based on the exponentiated average log scores, which has been called ``forecast skill'' and is equivalent to the geometric mean of the probabilities assigned to the eventually observed outcome. 
The geometric mean is an alternative measure of central tendency to an arithmetic mean, representing the $n^{th}$ root of a product of $n$ numbers. 
In this application, it has the appealing and intuitive interpretation of being the average probability assigned to the true outcome.

For example, the forecast skill for model $m$ and target $t$, averaged across the other dimensions,  would be calculated as
\begin{eqnarray}
 FS_{m,\cdot,t,\cdot,\cdot} & = & \exp \left ( \frac{1}{N} \sum_{r,s,w} \log \hat f_{m,r,t,s,w}(\hat z|{\bf x}) \right ) \\
 & = & \left ( \prod_{r,s,w} \hat f_{m,r,t,s,w}(\hat z|{\bf x}) \right ) ^{1/N} 
\end{eqnarray}
where $N$ is the total number of log-scores for target $t$ and model $m$, across all combinations of region, season, and week.
More generally, we will use the $\cdot$ subscript to indicate which indices are being summed over.
[[Occasionally, we will additionally take a further arithmetic mean across one of the indices, for example
\begin{eqnarray}
 FS_{m,r,t,\bar s,\cdot} & = &  \frac{1}{S} \sum_{s=1}^S FS_{m,r,t,s,\cdot}
\end{eqnarray}
where $S$ is the number of seasons
]]

Following the convention of the CDC challenges, we only included certain weeks in the calculation of the average forecast skill for each target.
Forecasts of season onset are evaluated based on the forecasts that are received up to six weeks after the observed onset week.
Forecasts of season peak and intensity are evaluated through the first forecast received after the weighted ILI goes below the regional baseline for the final time during a given season season. 
Week-ahead forecasts are evaluated using forecasts received four weeks prior to the onset week through forecasts received three weeks after the weighted ILI goes below the regional baseline for the final time.
[[In a region-season without an onset, all weeks are scored.]]

The CDC has used a modified log-score as a measure of forecast accuracy, and we adopt the same approach here.
The log-scores are computed for the targets on the wILI percentage scale such that predictions within +/- 0.5 percentage points are considered accurate, i.e. log score = $\log \int_{\hat z -.5}^{\hat z + .5} f^{(m)}(z|{\bf{x}})dz$. 
For the targets on the scale of epidemic weeks, predictions within +/- 1 week are considered accurate, i.e. log score = $\log \int_{\hat z -1}^{\hat z + 1} f^{(m)}(z|{\bf{x}})dz$. 
While this modification eliminates the advantage of the log-score being a proper scoring rule[[cite Gneiting]], it has the advantage of having a clear interpretation that is meaningful to public health officials.
To ensure all calculated summary measures would be finite, all modified log scores with values of less than -10 were assigned the value -10.

\subsection{Formal comparisons of model performance}

Model-based comparisons of forecast accuracy are hindered by the high correlation of sequential forecasts and by outlying observations. 
When observations assign no probability to the eventually observed outcome they have a log-score of $-\infty$.

%Things to confirm: removed weeks that CDC does not score, no onset seasons and multi peak years are handled appropriately

\section{Results}

\subsection{Comparing models' forecasting performance by season}
<<>>=
scores_by_model <- readRDS("./data/scores_by_model.rds")
kde_skill <- unlist(scores_by_model[which(scores_by_model$Model=="ReichLab-KDE"), "skill"])
max_skill <- max(scores_by_model$skill)
min_skill <- min(scores_by_model$skill)
max_skill_model <- as.character(unlist(scores_by_model[which(scores_by_model$skill == max_skill), "Model"]))
min_skill_model <- as.character(unlist(scores_by_model[which(scores_by_model$skill == min_skill), "Model"]))
n_above_kde <- sum(scores_by_model$skill>kde_skill)
n_models <- nrow(scores_by_model)

scores_by_season <- readRDS("./data/scores_by_season.rds")
tmp <- scores_by_season %>% group_by(Model) %>%
    summarize(n_seasons_above_maxavg = sum(skill>max_skill)) 
@


Averaging across targets and locations, forecast skill varied widely by model and season (Figure \ref{fig:results-model-season}). 
The historical baseline model showed an average seasonal skill of 
\Sexpr{specify_decimal(kde_skill, 2)}, 
meaning that in an typical season, across all targets and locations, this model assigned on average 
\Sexpr{specify_decimal(kde_skill, 2)} 
probability to the eventually observed value. 
The model with the highest average seasonal forecast skill 
({\tt \Sexpr{sanitize(max_skill_model)}}) 
and lowest 
({\tt \Sexpr{sanitize(min_skill_model)}}) 
had skills of \Sexpr{specify_decimal(max_skill, 2)} and \Sexpr{specify_decimal(min_skill, 2)}, respectively. 
Of the \Sexpr{n_models} models, \Sexpr{n_above_kde} models 
(\Sexpr{specify_decimal(n_above_kde/n_models*100)}\%) 
showed higher average seasonal forecast skill than the historical average.
Season-to-season variation was substantial, with 
\Sexpr{sum(tmp$n_seasons_above_maxavg>0)} 
models having at least one season with greater average forecast skill than the best model did.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/fig-results-model-season.pdf}
\caption{Average forecast skill, aggregated across targets and plotted separately for each model and season. Models are sorted from least skill (left) to most skill (right). Dots show average skill across all targets and regions for a given season. The x marks the average of the seven seasons. The names of compartmental models are shown in bold face. The ReichLab-KDE model can be thought of as the historical baseline model.}
\label{fig:results-model-season}
\end{center}
\end{figure}

The top six performing models utilized a range of methodologies, highlighting that very different approaches can result in very similar overall performance. 
The overall best model was an ensemble model ({\tt Delphi-Stat}) that used a weighted combination of other models from the Delphi group.
Both the {\tt ReichLab-KCDE} and the {\tt Delphi-DeltaDensity1} model utilized kernel conditional density estimation, a non-parametric statistical methodology that is a distribution-based variation on nearest-neighbors regression. 
These models used different implementations and different input variables, but showed similarly strong performance across all seasons.
The {\tt UTAustin-edm} and {\tt Delphi-DeltaDensity2} models also used variants of nearest-neighbors regression, although overall skill for these models was not as consistent, indicating that implementation details and/or input variables can impact the performance of this approach.
The {\tt LANL-DBM} and {\tt CU-EKF\_SIRS} models both rely on a compartmental model of influenza transmission, however the methodologies used to fit and forecast were different for these approaches.
The CU model used an ensemble-adjustment Kalman filter approach to generate forecasts, the LANL model used particle filtering.
The {\tt ReichLab-SARIMA2} model used a classical statistical time-series model, the seasonal auto-regressive integrated moving average model, to fit and generate forecasts. 
Interestingly, several pairs of models, although having strongly contrasting methodological approaches, showed similar overall performance; e.g., {\tt CU-EKF\_SIRS} and {\tt ReichLab-SARIMA2}, {\tt LANL-DBM} and {\tt ReichLab-KCDE}.

\subsection{Performance in forecasting week-ahead incidence}
<<>>=
scores_by_model_target <- readRDS("./data/scores_by_model_target.rds")
scores_by_model_targettype <- readRDS("./data/scores_by_model_targettype.rds")
scores_by_model_targettype_region <- readRDS("./data/scores_by_model_targettype_region.rds")

max_skill_wkahead <- max(scores_by_model_targettype$skill[which(scores_by_model_targettype$target_type == "k-week-ahead")])
max_skill_model_wkahead <- as.character(unlist(scores_by_model_targettype[which(scores_by_model_targettype$skill == max_skill_wkahead), "Model"]))[1] ## adding [1] in case multiple models are returned

kde_skill_wkahead <- unlist(scores_by_model_targettype[which(scores_by_model_targettype$Model=="ReichLab-KDE"& scores_by_model_targettype$target_type=="k-week-ahead"), "skill"])
n_above_kde_wkahead <- sum(scores_by_model_targettype$skill[which(scores_by_model_targettype$target_type=="k-week-ahead")]>kde_skill_wkahead)
@

Average forecast skill for all four week-ahead targets varied substantially across models and regions (Figure \ref{fig:results-model-region-tt}).
The model with the highest average week-ahead forecast skill across all regions and seasons was {\tt \Sexpr{sanitize(max_skill_model_wkahead)}}.
Within regions and across all seasons, this model achieved average forecast skill between
\Sexpr{specify_decimal(min(scores_by_model_targettype_region$skill[which(scores_by_model_targettype_region$Model==max_skill_model_wkahead & scores_by_model_targettype_region$target_type == "k-week-ahead" )]), 2)} 
and 
\Sexpr{specify_decimal(max(scores_by_model_targettype_region$skill[which(scores_by_model_targettype_region$Model==max_skill_model_wkahead & scores_by_model_targettype_region$target_type == "k-week-ahead" )]), 2)}.
As a comparison, the historical baseline model achieved between
\Sexpr{specify_decimal(min(scores_by_model_targettype_region$skill[which(scores_by_model_targettype_region$Model=="ReichLab-KDE" & scores_by_model_targettype_region$target_type == "k-week-ahead" )]), 2)} and
\Sexpr{specify_decimal(max(scores_by_model_targettype_region$skill[which(scores_by_model_targettype_region$Model=="ReichLab-KDE" & scores_by_model_targettype_region$target_type == "k-week-ahead" )]), 2)}
average skill for all week-ahead targets.
% Of the \Sexpr{n_models} models available for comparison to the historical baseline, \Sexpr{n_above_kde_wkahead}
% (\Sexpr{specify_decimal(n_above_kde_wkahead/n_models*100)}\%) showed greater skill than the historical baseline across all region-seasons.

<<results-model-region-tt, fig.cap="Average forecast skill by model region and target-type, averaged over weeks and seasons. The white midpoint of the color scale is set to be the overall average of the historical baseline model, ReichLab-KDE.">>=
midpt <- mean(filter(scores_by_model_targettype_region, Model=="ReichLab-KDE")$skill)
ggplot(scores_by_model_targettype_region, 
    aes(x=Location, fill=skill, y=Model)) + 
    geom_tile() + ylab(NULL) + xlab(NULL) +
    facet_grid(~target_type) +
    geom_text(aes(label=round(skill, 2)), size=2) +
    scale_fill_gradient2(midpoint = midpt, name="forecast skill") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
@


% \begin{figure}[htbp]
% \begin{center}
% \includegraphics[width=\textwidth]{figures/fig-results-region.pdf}
% \caption{Average forecast skill by model region and target-type, averaged over weeks and seasons. The white midpoint of the color scale is set to be the overall average of the historical baseline model, {\tt ReichLab-KDE}.}
% \label{fig:results-region}
% \end{center}
% \end{figure}


<<>>=
scores_by_model_season_targettype_region <- readRDS("./data/scores_by_model_season_targettype_region.rds")

## summarizing variation in target-type forecasts by model across season-regions
skillvar_by_mstr <- scores_by_model_season_targettype_region %>% 
    filter(Model != "Delphi-Uniform") %>% group_by(Model, target_type) %>%
    summarize(skillvar = var(skill))

min_skillvar_wkahead <- min(skillvar_by_mstr$skillvar[which(skillvar_by_mstr$target_type == "k-week-ahead")])
min_skillvar_model_wkahead <- as.character(unlist(skillvar_by_mstr[which(skillvar_by_mstr$skillvar == min_skillvar_wkahead), "Model"]))[1] ## adding [1] in case multiple models are returned

max_skillvar_wkahead <- max(skillvar_by_mstr$skillvar[which(skillvar_by_mstr$target_type == "k-week-ahead")])
max_skillvar_model_wkahead <- as.character(unlist(skillvar_by_mstr[which(skillvar_by_mstr$skillvar == max_skillvar_wkahead), "Model"]))[1] ## adding [1] in case multiple models are returned

## least variable model in weekahead forecasts
min_skillvar_model_wkahead_scores <- scores_by_model_season_targettype_region %>%
    filter(Model==min_skillvar_model_wkahead, target_type=="k-week-ahead") %>% 
    .$skill

## highest variable model in weekahead forecasts
max_skillvar_model_wkahead_scores <- scores_by_model_season_targettype_region %>%
    filter(Model==max_skillvar_model_wkahead, target_type=="k-week-ahead") %>% 
    .$skill

## top model region-season scores
top_model_wkahead_scores <- scores_by_model_season_targettype_region %>%
    filter(Model==max_skill_model_wkahead, target_type=="k-week-ahead") %>% 
    .$skill
@

Even within given models, target-specific forecast skill showed large region-to-region and year-to-year variation. 
The forecask skill for specific region-seasons shown by the high-accuracy {\tt \Sexpr{sanitize(max_skill_model_wkahead)}} model varied from 
\Sexpr{specify_decimal(min(top_model_wkahead_scores), 2)} to
\Sexpr{specify_decimal(max(top_model_wkahead_scores), 2)}.
The model with the lowest variation in combined week-ahead forecast skill across region-seasons (excluding the uniform model) was 
\Sexpr{sanitize(min_skillvar_model_wkahead)}, 
with skills ranging between 
\Sexpr{specify_decimal(min(min_skillvar_model_wkahead_scores), 2)} and 
\Sexpr{specify_decimal(max(min_skillvar_model_wkahead_scores), 2)}.
The model with the highest variation in forecast skill across region-seasons was 
\Sexpr{sanitize(max_skillvar_model_wkahead)}, 
with skills ranging between 
\Sexpr{specify_decimal(min(max_skillvar_model_wkahead_scores), 2)} and 
\Sexpr{specify_decimal(max(max_skillvar_model_wkahead_scores), 2)}.

<<>>=
## Get models better than KDE for k-week-ahead
scores_wkahead_better_than_kde <- scores_by_model_targettype %>%
    filter(target_type=="k-week-ahead", skill>kde_skill_wkahead)
models_wkahead_better_than_kde <- as.character(scores_wkahead_better_than_kde$Model)

## look at region-specific, k-week-ahead scores for those models
scores_aggr_by_region <- scores_by_model_targettype_region %>% 
    filter(Model %in% models_wkahead_better_than_kde, target_type=="k-week-ahead") %>%
    group_by(Location) %>%
    summarize(avg_skill = mean(skill))
best_region <- scores_aggr_by_region$Location[which.max(scores_aggr_by_region$avg_skill)]
best_region_skill <- scores_aggr_by_region$avg_skill[which.max(scores_aggr_by_region$avg_skill)]
worst_region <- scores_aggr_by_region$Location[which.min(scores_aggr_by_region$avg_skill)]
worst_region_skill <- scores_aggr_by_region$avg_skill[which.min(scores_aggr_by_region$avg_skill)]

kde_kwkahead_worst_score <- scores_by_model_targettype_region %>% 
    filter(Model %in% "ReichLab-KDE", Location==worst_region, target_type=="k-week-ahead") %>% 
    .$skill
@

Models were more consistently able to forecast week-ahead wILI in some regions than in others.
Looking at results from the \Sexpr{length(models_wkahead_better_than_kde)} models that on average had more skill than the historical baseline in forecasting week-ahead targets, \Sexpr{best_region} was the most predictable and \Sexpr{worst_region} was the least predictable.
In \Sexpr{best_region}, the subset of better-than-historical-average models showed an average forecast skill of \Sexpr{specify_decimal(best_region_skill, 2)} (Figure \ref{fig:map-results}). 
This means that in a typical season these models together assigned an average of \Sexpr{specify_decimal(best_region_skill, 2)} probability to the eventually observed outcome.
On the flip side, in \Sexpr{worst_region} the average skill was \Sexpr{specify_decimal(worst_region_skill, 2)}. 
This was just marginally better than the forecast skill for the historical average model in \Sexpr{worst_region}, which was \Sexpr{specify_decimal(kde_kwkahead_worst_score,2)}.
<<map-results, fig.cap="Map figure. Color indicates average forecast skill for models that performed better than the historical average model.">>=
data("fifty_states")
data(hhs_regions)
scores_for_map <- scores_aggr_by_region %>% 
    filter(Location != "US National")
scores_for_map$region_number <- gsub("[^0-9]", "", scores_for_map$Location)
not_50 <- c(9, 10, 12, 50:55)
hhs_regions <- hhs_regions[-(not_50),]
hhs_regions$state <- tolower(hhs_regions$state_or_territory)
skill_lims <- c(.1, .6)

wkahead_top <- hhs_regions %>% 
    merge(scores_for_map, by="region_number") %>%
    ggplot(aes(map_id = state)) + 
    geom_map(aes(fill = avg_skill), map = fifty_states) +
    scale_fill_gradient(low="#f7fbff", high="#084594", limit=skill_lims) + 
    expand_limits(x = fifty_states$long, y = fifty_states$lat) +
    coord_map() +
    annotate("text", x = -118, y =  45, label = "10") +
    annotate("text", x = -119.48333, y =  40, label = "9") +
    annotate("text", x = -105.48333, y =  44, label = "8") +
    annotate("text", x = -92.1, y =  45, label = "5") +
    annotate("text", x = -97, y =  40, label = "7") +
    annotate("text", x = -101, y =  35, label = "6") +
    annotate("text", x = -85, y =  35, label = "4") +
    annotate("text", x = -78.4, y =  38.5, label = "3") +
    annotate("text", x = -76, y =  42.75, label = "2") +
    annotate("text", x = -69.5, y =  44.95, label = "1") +
    scale_x_continuous(breaks = NULL) + 
    scale_y_continuous(breaks = NULL) +
    labs(x = NULL, y = NULL) +
    ggtitle("A: week-ahead forecast skill") #+
    # theme(legend.position = "none", 
    #     panel.background = element_blank())


kde_scores <- scores_by_model_targettype_region %>%
    filter(Model %in% "ReichLab-KDE", target_type=="k-week-ahead") %>%
    rename(kde_skill = skill) %>% 
    left_join(scores_aggr_by_region) %>%
    mutate(skill_diff = avg_skill - kde_skill)

#wkahead_comparison <- ggplot(kde_scores, aes(x=Location, y=skill_diff)) + 
#    geom_point(aes(color=avg_skill)) +
#    geom_hline(yintercept=0, linetype=2)

hhs_regions %>% 
    merge(kde_scores, by="region_number") %>%
    ggplot(aes(map_id = state)) + 
    geom_map(aes(fill = skill_diff), map = fifty_states) +
    scale_fill_gradient(low="#f7fbff", high="#084594", limit=skill_lims) + 
    expand_limits(x = fifty_states$long, y = fifty_states$lat) +
    coord_map() +
    annotate("text", x = -118, y =  45, label = "10") +
    annotate("text", x = -119.48333, y =  40, label = "9") +
    annotate("text", x = -105.48333, y =  44, label = "8") +
    annotate("text", x = -92.1, y =  45, label = "5") +
    annotate("text", x = -97, y =  40, label = "7") +
    annotate("text", x = -101, y =  35, label = "6") +
    annotate("text", x = -85, y =  35, label = "4") +
    annotate("text", x = -78.4, y =  38.5, label = "3") +
    annotate("text", x = -76, y =  42.75, label = "2") +
    annotate("text", x = -69.5, y =  44.95, label = "1") +
    scale_x_continuous(breaks = NULL) + 
    scale_y_continuous(breaks = NULL) +
    labs(x = NULL, y = NULL) +
    ggtitle("A: week-ahead forecast skill")

grid.arrange(wkahead_top, wkahead_comparison, ncol=2)
@



<<>>=
scores_by_model_season_target_region <- readRDS("./data/scores_by_model_season_target_region.rds")
kde_skill_1wkahead <- scores_by_model_target %>% 
    filter(Target=="1 wk ahead", Model=="ReichLab-KDE") %>%
    .$skill
n_models_better_than_kde_1wk <- scores_by_model_target %>%
    filter(Target=="1 wk ahead", skill>kde_skill_1wkahead) %>%
    nrow() 
kde_skill_4wkahead <- scores_by_model_target %>% 
    filter(Target=="4 wk ahead", Model=="ReichLab-KDE") %>%
    .$skill
n_models_better_than_kde_4wk <- scores_by_model_target %>%
    filter(Target=="4 wk ahead", skill>kde_skill_4wkahead) %>%
    nrow() 

regions_above_.5_best_model <- scores_by_model_season_target_region %>%
    filter(Target %in% paste(1:2, "wk ahead"), Model==max_skill_model_wkahead) %>%
    group_by(Location, Target) %>%
    summarize(avg_skill = mean(skill)) %>%
    ungroup() %>%
    group_by(Location) %>%
    summarize(both_above_.5 = all(avg_skill>=.5)) %>%
    filter(both_above_.5) %>% .$Location
@

Forecast skill declined as the target moved further into the future.
For the model with highest forecast skill across all four week-ahead targets ({\tt \Sexpr{sanitize(max_skill_model_wkahead)}}), the average skill across region and season for 1 through 4 week-ahead forecasts were 
\Sexpr{specify_decimal(scores_by_model_target$skill[which(scores_by_model_target$Model==max_skill_model_wkahead & scores_by_model_target$Target == "1 wk ahead" )], 2)}, 
\Sexpr{specify_decimal(scores_by_model_target$skill[which(scores_by_model_target$Model==max_skill_model_wkahead & scores_by_model_target$Target == "2 wk ahead" )], 2)}, 
\Sexpr{specify_decimal(scores_by_model_target$skill[which(scores_by_model_target$Model==max_skill_model_wkahead & scores_by_model_target$Target == "3 wk ahead" )], 2)}, and 
\Sexpr{specify_decimal(scores_by_model_target$skill[which(scores_by_model_target$Model==max_skill_model_wkahead & scores_by_model_target$Target == "4 wk ahead" )], 2)}.
This mirrored an overall decline in skill observed across most models.
Only in \Sexpr{paste(regions_above_.5_best_model, collapse=" and ")} were the forecast skills from the {\tt \Sexpr{sanitize(max_skill_model_wkahead)}} model for both the ``nowcast'' targets (1 and 2 weeks ahead) above 0.5.
The historical baseline model showed average forecast skill of 
\Sexpr{specify_decimal(scores_by_model_target$skill[which(scores_by_model_target$Model=="ReichLab-KDE" & scores_by_model_target$Target == "1 wk ahead" )], 2)}, 
for all week-ahead targets. 
(Performance does not decline for the historical model, since it always forecasts the same thing for every week, without updating based on recent data.)
For 1 week-ahead forecasts, \Sexpr{n_models_better_than_kde_1wk} of \Sexpr{n_models} models (\Sexpr{specify_decimal(n_models_better_than_kde_1wk/n_models*100)}\%) showed more skill than a historical baseline.
For the 4 week-ahead forecasts, only \Sexpr{n_models_better_than_kde_4wk} of \Sexpr{n_models} models (\Sexpr{specify_decimal(n_models_better_than_kde_4wk/n_models*100)}\%) showed more skill than the historical baseline.

\subsection{Performance in forecasting seasonal targets}

Of the three seasonal targets, models showed the lowest average skill in forecasting season onset, with an overall average skill of XX. 
Due to the variable timing of season onset, different numbers of weeks were included in the final scoring for each region-season, varying from XX to XX weeks per region-season (see methods for details).
Of the XX region-seasons evaluated, XX had no onset. 
The best model for onset was XX, with overall average skill of XX and minimum skill for a region-season of XX.
Overall, XX of XX models had more forecast skill than the historical baseline model in the scoring period of interest.


Models showed less overall skill in forecasting the peak week and peak intensity when compared to the week-ahead forecasts.
Model-specific average skill for peak week, across region and season, was above 0.25 for XX models. 
This means that XX models assigned on average at least 25\% probability to a week within +/- 1 week of the eventually observed peak week during the scoring period of interest.
Similarly, for peak intensity, XX models assigned on average at least 25\% probability to a wILI percentage within +/- 0.5\% of the eventually observed value during the scoring period of interest.
During the same time-periods, the historical model forecasts assigned XX\% probability to the eventually observed peak week and XX\% probability to the eventually observed peak intensity.

Average forecast skill showed substantial variation by region and by season.
[[Forecasts of peak intensity and peak week showed lower skill in seasons that experienced higher than average peak incidence.]]
[[Forecasts of peak intensity and peak week showed lower skill in regions with larger variability in peak incidence.]]



\subsection{Performance of models by location}

Consider adding map that averages across all models, or shows max skill per region, as it varies quite a bit.

two columns: by target type
mapa: average performance across all models(-KDE)/targets by region
mapb: performance of historical model by region
figc: scatter plot of AVERAGE region-level historical model skill on x and avg model skill on y (one point for each region, aggregated across seasons?). Point is to show that regions with higher season-to-season variation (i.e. lower historical forecast skill) also have lower skill from other models? Maybe plot one line for top models, one for all models. 

Hypothesis: year-to-year variation within a region (as measured by skill -> lower skill = larger variation) is associated with skill of (top) models for that region. 
lower skill = larger variation --> lower component model skill


\subsection{Comparison between statistical and compartmental models}


Statistical models showed the same amount of skill as compartmental models at forecasting week-ahead targets, and slightly more skill for the seasonal targets. 
Using the best three overall models from each category, we computed the average forecast skill for each combination of region, season, and target (Table \ref{tab:score-by-model-type}). 
For the week-ahead forecasts, the difference in model skill was below XX.
For the three seasonal targets, the difference in model skill was larger, ranging from XX for [target X] to XX for [target X].
We note that the 1 week-ahead forecasts from the compartmental models from the CU team are driven largely by a statistical ``nowcast`` model that uses data from the Google Search API and influenza laboratory testing data from the CDC to create the ILI+ metric.\cite{yang2014}
Therefore, the only compartmental model making 1 week-ahead forecasts is the LANL-DBM model. 

\input{./static-figures/score-by-model-type.tex}


% Gamma mixed effects regression model on negative log-scores. Need to remove correlation between successive observation.
% 
% strategies to remove correlation between successive scores within a year:
% \begin{itemize}
%     \item smooth/aggregate scores into different sections of the year, possibly reducing the number of and/or correlation between sucessive scores
%     \item smooth spline across years with random effect for model, fixed effects for model-type
%     \item permutation test
% \end{itemize}


\subsection{Where do these models fail?}

In addition to examining where our models perform well, we also identified situations in which where current state-of-the-art forecast models still need improvement. 
We identify and quantify several of these challenges, including revisions to initially reported data, ....

\subsubsection*{Delayed case reporting impacts forecast skill}
When the first report of the wILI measurement for a given region-week is not accurate (due to incomplete or delayed reporting), this has a strong negative impact on forecast accuracy.
In the seven years examined in this study, wILI percentages were often revised after first being reported. 
For example, XX\% of all weekly reported wILI percentages ended up being over 20\% different than the originally reported value.
We found that an increase in the bias of the initially reported data was stronly associated with a decrease in the forecast skill for the forecasts made using the biased data.
Specifically, among top-performing models we see a XX decline in forecast skill when the first observed wILI measurement is XX, adjusting for model, week-of-year, and target (Figure \ref{fig:delay-model-coefs}).
These results are based on results from four top-performing models: ReichLab-KCDE, LANL-DBM, Delphi-DeltaDensity1, and CU-EKF\_SIRS.
This pattern is symmetric for under- and over-reported values, although there are more extreme under-reported values than there are over-reported values. 


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/fig-delay-model-coefs.pdf}
\caption{Model-estimated changes in forecast skill due to bias in initial reports of wILI \%. The figure shows estimated coefficient values (and 95\% confidence intervals) from a multivariate linear regression using model, week-of-year, target, and a categorized version of the bias in the first reported wILI \% to predict forecast skill. The model was fit to  The x-axis labels show the range of bias (e.g. ``(-0.5,0.5]'' represents all observations whose first observations were within +/- 0.5 percentage points of the final reported value). Values to the left of the dashed grey line are observations whose first reported value were lower than the final. Y-axis values of less than zero (the reference category) represent decreases in expected forecast skill. The total number of observations in each category are shown above the x-axis labels.s}
\label{fig:delay-model-coefs}
\end{center}
\end{figure}


\subsubsection*{Intensity of season not reliably correlated with forecast skill}

We anticipated seeing a relationship between the peak intensity of the season and the observed forecast skill for the peak. 
However, no clear relationship between the peak intensity of the season and the skill at forecasting the peak intensity was observed (data not shown).
% See analysis in code/peak-analysis.R.

% [[Add a paragraph about capturing a holiday spike?]]

% \begin{itemize}
%     \item holiday peak?
%     \item missing a slow descent because of a second outbreak that is lifting the tail up?
% \end{itemize}



\section{Discussion}

\subsection{Overview of key results and importance}
The first large-scale comparison of flu forecasting models from different modeling teams/philosophies across multiple years.

\subsection{Overview of statistical vs. mechanistic model comparison}
As our knowledge/data about the system mature, we expect mechanistic models to be better, but when true signals of mechanistic model is drowned out by observational noise or spatial aggregation, statistical models may perform better. This comparison serves as a barometer for where the current state of forecast models are.

\subsection{Limitations}

\begin{itemize}
    \item relatively few additional data sources incorporated
    \item no models that explicitly incorporate strain information
    \item no models with spatial information included
    \item seven seasons of data is not a lot (n=7) to draw strong conclusions about comparative model performance
    \item currently limited to models with only recent data...
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{../flusightnetwork.bib}

\end{document}