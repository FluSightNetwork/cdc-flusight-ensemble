\documentclass{article}

\title{Comparing Mechanistic and Statistical Models to Forecast Influenza in the U.S.}

\author{Logan Brooks, Spencer Fox, Craig McGowan, Sasikiran Kandula, \\ Dave Osthus, Evan Ray, Nicholas G Reich, Roni Rosenfeld, Jeffrey Shaman, \\Abhinav Tushar, Teresa Yamana [authorship list to be finalized]}

\usepackage[letterpaper, margin=1in]{geometry} % margin
\usepackage{lineno}% add line numbers
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{parskip}        % for spacing after paragraphs http://ctan.org/pkg/parskip
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath, amsfonts}
\usepackage{setspace}
\linenumbers % line numbers
\onehalfspacing



% For computer modern sans serif
\usepackage[T1]{fontenc}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif


\begin{document}

\maketitle

\tableofcontents

<<echo=FALSE, warning=FALSE, message=FALSE>>=
knitr::opts_chunk$set(
  echo = FALSE, cache = TRUE, cache.path = './cache', message = FALSE, warning = FALSE
)
library(dplyr)
library(readr)
@


\section{Introduction}

In recent years, the quantity of research on forecasting infectious diseases has increased XX fold. 
This increased interest has been fueled in part by the promise of `big data', that near real-time data streams of everything from large-scale population behavior to microscopic patterns in disease transmission could lead to measurable improvements in how disease transmission is detected and prevented. 
With the spectre of a serious pandemic looming over global public health preparedness efforts and national security planning, efforts to improve infectious disease forecasting efforts continue to be a central concern of global health, national security, and broader geopolitical stability.

Forecasts of infectious disease outbreaks can inform public health response to outbreaks. 
Accurate forecasts of the timing and spatial spread of seasonal outbreaks of diseases such as influenza or dengue fever can provide valuable information about where public health interventions can be targeted.
Decisions about hospital staffing, resource allocation, and the timing of public health communication campaigns could be assisted by forecasts. 
Implementation of interventions designed to disrupt disease transmissions, such as vector control measures or mandatory infection prevention protocols at hospitals or helath clinics, could be targeted based on forecasted incidence.

Public health officials are still learning how to best integrate forecasts into real-time decision making.
Close collaboration between public health policy-makers and quantitative modelers is necessary to ensure the forecasts have maximum impact and are appropriately communicated to the public and the broader public health community. 
Understanding what targets should be forecasted for maximum public health impact is hard to assess without real-time implementation and testing.


Starting in the 2013-2014 influenza season, the U.S. Centers for Disease Control and Prevention (CDC) has run the "Forecast the Influenza Season Collaborative Challenge" (a.k.a. FluSight) each influenza season, soliciting weekly forecasts for specific influenza season metrics from teams across the world.
These forecasts are displayed together on a website during the season and are evaluated for accuracy after the season is over.\cite{PhiResearchLab} 
This effort has galvanized a community of scientists interested in forecasting, creating an organic testbed for improving both our technical understanding of how different forecast models perform but also how to integrate these models into decsision-making.

Building on the structure of the FluSight challenges (and those of other collaborative forecasting efforts\cite{Viboud}), a subset of participants founded a consortium to facilitate direct comparison and fusion of modeling approaches. 
In this paper, we provide a detailed analysis of the performance of 22 different models from 5 different teams over the course of seven influenza seasons.
Drawing on the different expertise of the five teams allows us to make fine-grained and standardized comparisons of distinct modeling approaches that using different data sources.
Additionally, it allows us to identify gaps and continued challenges that should be addressed in future modeling efforts. 



% Infectious disease modeling has proven to be fertile ground for statisticians, mathematicians, and quantitative modelers for over a century. 
% Yet there is not a consensus on a single best modeling approach or method for forecasting the dynamic patterns of infectious disease outbreaks, in both endemic and emergent settings. 
% Mechanistic models consider the biological underpinnings of disease transmission, and are in practice are typically implemented as variations on the Susceptible-Infectious-Recovered (SIR) model. 
% Phenomenological models largely ignore the biological underpinnings and theory of disease transmission and focus instead on using data-driven, empirical and statistical approahces to make the best forecasts possible of a given dataset, or phenomenon. 
% Both approaches are commonly used and both have advantages and disadvantages in different settings.   


\section{Methods}

\subsection{FluSight Challenge Overview}
 
Detailed methodology and results from previous FluSight challenges have been published\cite{Biggerstaff2016}, but we summarize the key features of the challenge here.

The FluSight challenge has been focused on forecasts of the weighted percentage of doctor's office visits for influenza-like-illness (wILI) in a particular region. This is a standard measure of seasonal flu activity, for which public data is available back to the 1997/1998 influenza season. During each influenza season, this data is updated each week by the CDC (Figure \ref{fig:timezero-schematic}). When the most recent data is released, the prior weeks' reported wILI data may also be revised. The unrevised data, available at a particular moment in time, is available via the DELPHI real-time epidemiological data API beginning in the 2013/2014 season.\cite{DELPHI} This API enables researchers to ``turn back the clock'' to a particular moment in time and use the data available at that time. This enables more accurate assessment of how models would have performed in real-time. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{static-figures/timezero-sketch.pdf}
\caption{A schematic showing when data arrives in realtime relative to when the forecasts are made available. Based on typical timelines for the CDC FluSight challenge, we assume that forecasts are generated or submitted to the CDC using the most recent reported data. This data includes the first reported observations of wILI\% from two weeks prior. Therefore, 1 and 2 week-ahead forecasts are considered nowcasts, i.e. at or before the current time. Similarly, 3 and 4 week-ahead forecasts are considered proper forecasts, or estimates about events in the future.}
\label{fig:timezero-schematic}
\end{center}
\end{figure}


The FluSight challenges have defined seven forecasting targets of particular public health relevance. Three of these targets are fixed scalar values for a particular season: onset week, peak week, and peak intensity (i.e. the maximum observed wILI percentage). The remaining four targets are the observed wILI percentages in each of the subsequent four weeks. 

The FluSight challenges have also required that all forecast submissions and follow a particular format. A single submission file (a comma-separated text file) contains the forecast made for a particular epidemic week (EW) of a season. Standard CDC definitions of epidemic week are used. Each file contains binned predictive distributions for seven specific targets across the 10 HHS regions of the US plus the national level. Each file contains over 8000 rows and typically is about 400KB in size.

To be included in the model comparison presented here, previous participants in the CDC FluSight challenge were invited to provide out-of-sample forecasts for the 2010/2011 through 2016/2017 seasons. For each model, this involved creating 233 separate forecast submission files, one for each of the weeks in the seven training seasons.
Each forecast file represented a single submission file, as would be submitted to the CDC challenge. 
Each team created their submitted forecasts in a prospective, out-of-sample fashion, i.e. fitting or training the model only on data available before the time of the forecast (see Figure \ref{fig:timezero-schematic}). 

\subsection{Description of this Forecasting Experiment}


\subsection{Summary of Models}

Five teams each submitted between 1 and 9 separate models for evaluation (Table \ref{tab:model-list}). 
A wide range of methodological approaches and modeling paradigms are included in the set of forecast models.
For example, seven of the models utilize a compartmental structure (e.g. Susceptible-Infectious-Recovered), a model framework that directly encodes both the transmission and the susceptible-limiting dynamics of infectious disease outbreaks.
Other less directly mechanistic models, use statistical approaches to model the outbreak phenomenon directly by incorporating recent incidence and seasonal trends.
XX models directly incorporate external data (i.e. not just the wILI measurements from the CDC ILINet dataset), including historical humidity data and Google search data.
Two models stand out as being clear na\"ive baseline models, that never change based on recent data. 
The Delphi-Uniform model always provides a forecast that assigns equal probability to all possible outcomes. 
The ReichLab-KDE model yields predictive distributions based entirely on data from other seasons using kernel density estimation (KDE) for seasonal targets and a generalized additive model with cyclic penalized splines for weekly incidence.


\input{./static-figures/model-table.tex}

\subsection{Metrics Used for Evaluation and Comparison}

Forecasts have historically been evaluated by the CDC using two metrics, the log-score and the mean absolute error. These two metrics capture different desirable features of performance. The log-score enables evaluation of both the precision and accuracy of a forecast, using the predicted density function.\cite{Gneiting2007} The absolute error provides an interpretable summary of the amount of error the point estimates had on average.\cite{Reich2016} 

We used a modified form of the log-score to evaluate forecasts, in line with the evaluation performed by the CDC. The log-score is defined as $\log f(\hat z|\bf{x})$ where $f(z|\bf{x})$ is a predictive density function for some target $z$, conditional on some data $\bf{x}$ and $\hat z$ is the observed value of the target $z$. In practice, each model $m$ has a set of log scores associated with it are region-, target-, season-, and week- specific, notated as $\log f^{(m)}_{r,t,s,w}(\hat z|\bf{x})$. We evaluated model performance based on the exponentiated average log scores, which has been called ``forecast skill'' and is equivalent to the geometric mean of the probabilities assigned to the eventually observed outcome. 
For example, the forecast skill for model $m$ and target $t$ would be calculated as
\begin{eqnarray}
 FS^{m}_{t} & = & \exp \left ( \frac{1}{N} \sum_{r,s,w} \log \hat f^{(m)}_{r,t,s,w}(\hat z|{\bf x}) \right ) \\
 & = & \left ( \prod_{r,s,w} \hat f^{(m)}_{r,s,w}(\hat z|{\bf x}) \right ) ^{1/N} 
\end{eqnarray}
where $N$ is the total number of log-scores for target $t$ and model $m$, across all combinations of region, season, and week. 
Further, within a given region-season-target combination, the weeks included in the calculation of the average forecast skill depend on when the onset and peak occur. Specifically, [[...]].
All weeks are included for the forecast skill calculations for the $k$-step ahead forecasts of wILI.

The log-scores are computed for the targets on the wILI percentage scale such that predictions within +/- 0.5 percentage points are considered accurate, i.e. log score = $\log \int_{\hat z -.5}^{\hat z + .5} f^{(m)}(z|{\bf{x}})dz$. For the targets on the scale of epidemic weeks, predictions within +/- 1 week are considered accurate, i.e. log score = $\log \int_{\hat z -1}^{\hat z + 1} f^{(m)}(z|{\bf{x}})dz$. 
\begin{itemize}
    \item log-score for predictive distribution, aggregated by (model), (model x season), (model x season x location), (model x season x target-type), (model x season x target), , (model x season x week)
    \item MAE for point predictions
\end{itemize}

\subsection{Formal comparisons of model performance}

Model-based comparisons of forecast accuracy are hindered by the high correlation of sequential forecasts and by outlying observations. 
When observations assign no probability to the eventually observed outcome they have a log-score of $-\infty$.

%Things to confirm: removed weeks that CDC does not score, no onset seasons and multi peak years are handled appropriately

\section{Results}

\subsection{Summary of forecast skill by season}
<<>>=
scores_by_model <- readRDS("./data/scores_by_model.rds")
kde_skill <- round(unlist(scores_by_model[which(scores_by_model$Model=="ReichLab-KDE"), "skill"]),2)
max_skill <- max(scores_by_model$skill)
min_skill <- min(scores_by_model$skill)
max_skill_model <- as.character(unlist(scores_by_model[which(scores_by_model$skill == max_skill), "Model"]))
min_skill_model <- as.character(unlist(scores_by_model[which(scores_by_model$skill == min_skill), "Model"]))
n_above_kde <- sum(scores_by_model$skill>kde_skill)
n_models <- nrow(scores_by_model)

scores_by_season <- readRDS("./data/scores_by_season.rds")
tmp <- scores_by_season %>% group_by(Model) %>%
    summarize(n_seasons_above_maxavg = sum(skill>max_skill)) 
@


Averaging across targets and locations, forecast skill varied widely by model and season (Figure \ref{fig:results-season}). 
The historical baseline model showed an average seasonal skill of 
$\Sexpr{kde_skill}$, 
meaning that in an typical season, across all targets and locations, this model assigned on average 
$\Sexpr{kde_skill}$ 
probability to the eventually observed value. 
The model with the highest average seasonal forecast skill 
({\tt \Sexpr{max_skill_model}}) 
and lowest 
({\tt \Sexpr{min_skill_model}}) 
had skills of $\Sexpr{round(max_skill, 2)}$ and $\Sexpr{round(min_skill, 2)}$, respectively. 
Of the $\Sexpr{n_models}$ models, $\Sexpr{n_above_kde}$ models 
($\Sexpr{round(n_above_kde/n_models*100)}$\%) 
showed higher average seasonal forecast skill than the historical average.
Season-to-season variation was substantial, with 
$\Sexpr{sum(tmp$n_seasons_above_maxavg>0)}$ 
models having at least one season with greater forecast skill than the best model did on average.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/fig-results-model-season.pdf}
\caption{Model forecast season, overall and by season. Models are sorted from least skill (left) to most skill (right). Dots show average skill across all targets and regions for a given season. The x marks the average of the seven seasons. The names of compartmental models are shown in bold face. The ReichLab-KDE model can be thought of as the historical baseline model.}
\label{fig:results-season}
\end{center}
\end{figure}

The top six performing models utilized a range of methodologies, highlighting that very different approaches can result in very similar overall performance. 
The overall best model was an ensemble model (Delphi-Stat) that used a weighted combination of other models from the Delphi group.
Both the ReichLab-KCDE and the Delphi-DeltaDensity1 model utilized kernel conditional density estimation, a non-parametric statistical methodology that is distribution-based variation on nearest-neighbors regression. 
These models used different implementations and different input variables, but showed similarly strong performance across all seasons.
The UTAustin-edm and Delphi-DeltaDensity2 models also used variants of nearest-neighbors regression, although overall skill for these models was not as consistent, indicating that implementation and or input variables can impact the performance of this approach.
The LANL-DBM and CU-EKS\_SIRS models both rely on a compartmental model of influenza transmission, however the methodologies used to fit and forecast were different for these approaches.
The CU model used an ensemble-adjustment Kalman filter approach to generate forecasts, the LANL model used particle filtering.
The ReichLab-SARIMA2 model used a classical statistical time-series model, the seasonal auto-regressive integrated moving average model, to fit and generate forecasts. 
Interestingly, several pairs of models, although having strongly contrasting methodological approaches, showed similar overall performance; e.g., CU-EKF\_SIRS and ReichLab-SARIMA2, LANL-DBM and ReichLab-KCDE.

\subsection{Performance in forecasting week-ahead incidence}

Average forecast skill for the four week-ahead targets varied substantially across models and regions.
The best model (XX) achieved between XX and XX average skill in forecasting week-ahead targets across all seasons and regions.
The historical baseline model achieved between XX and XX average skill.
Of the XX model-region-seasons available for comparison to the historical baseline, XX\% showed greater skill than the historical baseline for a given region-season.


Even within given models, forecast skill showed large region-to-region and year-to-year variation. 
The model with the lowest variation in forecast skill across region-seasons was XX, with skills ranging between XX and XX.
The model with the highest variation in forecast skill across region-seasons was XX, with skills ranging between XX and XX.
In general, Region XX was the easiest to forecast and XX was the hardest, with models showing an average forecast skill of XX and XX across all seasons, respectively.

Forecast skill declined as the target moved further into the future.
For the model with highest forecast skill across all four week-ahead targets (XX), the average skill across region and season for 1 through 4 week-ahead forecasts were XX, XX, XX and XX.
This mirrored an overall decline in skill observed across most models.
The historical baseline model showed average forecast skill of XX for all week-ahead targets. (Performance does not decline for the historical model, since it always forecasts the same thing for every week, without updating based on recent data.)
For 1 week-ahead forecasts, XX of 21 models showed more skill than a historical baseline. 
For the 4 week-ahead forecasts, only XX of 21 models showed more skill than the historical baseline.
In Regions XX, XX, and XX, the average forecast skill for the ``nowcast'' targets (1 and 2 weeks ahead) were both above 0.5.


\subsection{Performance in forecasting seasonal targets}

Of the three seasonal targets, models showed the lowest average skill in forecasting season onset, with an overall average skill of XX. 
Due to the variable timing of season onset, different numbers of weeks were included in the final scoring for each region-season, varying from XX to XX weeks per region-season (see methods for details).
Of the XX region-seasons evaluated, XX had no onset. 
The best model for onset was XX, with overall average skill of XX and minimum skill for a region-season of XX.
Overall, XX of XX models had more forecast skill than the historical baseline model in the scoring period of interest.


Models showed less overall skill in forecasting the peak week and peak intensity when compared to the week-ahead forecasts.
Model-specific average skill for peak week, across region and season, was above 0.25 for XX models. 
This means that XX models assigned on average at least 25\% probability to a week within +/- 1 week of the eventually observed peak week during the scoring period of interest.
Similarly, for peak intensity, XX models assigned on average at least 25\% probability to a wILI percentage within +/- 0.5\% of the eventually observed value during the scoring period of interest.
During the same time-periods, the historical model forecasts assigned XX\% probability to the eventually observed peak week and XX\% probability to the eventually observed peak intensity.

Average forecast skill showed substantial variation by region and by season.
[[Forecasts of peak intensity and peak week showed lower skill in seasons that experienced higher than average peak incidence.]]
[[Forecasts of peak intensity and peak week showed lower skill in regions with larger variability in peak incidence.]]



\subsection{Performance of models by location}

Consider adding map that averages across all models, or shows max skill per region, as it varies quite a bit.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/fig-results-region.pdf}
\caption{Model results by region and target-type.}
\label{fig:results-region}
\end{center}
\end{figure}


two columns: by target type
mapa: average performance across all models(-KDE)/targets by region
mapb: performance of historical model by region
figc: scatter plot of region-level historical model skill on x and avg model skill on y, colored by season


\subsection{Comparison between statistical and compartmental models}


Statistical models showed the same amount of skill as compartmental models at forecasting week-ahead targets, and slightly more skill for the seasonal targets. 
Using the best three overall models from each category, we computed the average forecast skill for each combination of region, season, and target (Table \ref{tab:score-by-model-type}). 
For the week-ahead forecasts, the difference in model skill was below XX.
For the three seasonal targets, the difference in model skill was larger, ranging from XX for [target X] to XX for [target X].
We note that the 1 week-ahead forecasts from the compartmental models from the CU team are driven largely by a statistical ``nowcast`` model that uses data from the Google Search API and influenza laboratory testing data from the CDC to create the ILI+ metric.\cite{yang2014}
Therefore, the only truly compartmental model making 1 week-ahead forecasts is the LANL-DBM model. 

\input{./static-figures/score-by-model-type.tex}


% Gamma mixed effects regression model on negative log-scores. Need to remove correlation between successive observation.
% 
% strategies to remove correlation between successive scores within a year:
% \begin{itemize}
%     \item smooth/aggregate scores into different sections of the year, possibly reducing the number of and/or correlation between sucessive scores
%     \item smooth spline across years with random effect for model, fixed effects for model-type
%     \item permutation test
% \end{itemize}


\subsection{Where do these models fail?}

In addition to examining where our models perform well, we also identified situations in which where current state-of-the-art forecast models still need improvement. 
We identify and quantify several of these challenges, including revisions to initially reported data, ....

When the first report of the wILI measurement for a given region-week is not accurate (due to incomplete or delayed reporting), this has a strong negative impact on forecast accuracy.
In the seven years examined in this study, wILI percentages were often revised after first being reported. 
For example, XX\% of all weekly reported wILI percentages ended up being over 20\% different than the originally reported value.
We found that an increase in the bias of the initially reported data was stronly associated with a decrease in the forecast skill for the forecasts made using the biased data.
Specifically, among top-performing models we see a XX decline in forecast skill when the first observed wILI measurement is XX, adjusting for model, week-of-year, and target (Figure \ref{fig:delay-model-coefs}).
These results are based on results from four top-performing models: ReichLab-KCDE, LANL-DBM, Delphi-DeltaDensity1, and CU-EKF\_SIRS.
This pattern is symmetric for under- and over-reported values, although there are more extreme under-reported values than there are over-reported values. 


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{figures/fig-delay-model-coefs.pdf}
\caption{Model-estimated changes in forecast skill due to bias in initial reports of wILI \%. The figure shows estimated coefficient values (and 95\% confidence intervals) from a multivariate linear regression using model, week-of-year, target, and a categorized version of the bias in the first reported wILI \% to predict forecast skill. The model was fit to  The x-axis labels show the range of bias (e.g. ``(-0.5,0.5]'' represents all observations whose first observations were within +/- 0.5 percentage points of the final reported value). Values to the left of the dashed grey line are observations whose first reported value were lower than the final. Y-axis values of less than zero (the reference category) represent decreases in expected forecast skill. The total number of observations in each category are shown above the x-axis labels.s}
\label{fig:delay-model-coefs}
\end{center}
\end{figure}


We anticipated seeing a relationship between the peak intensity of the season and the observed forecast skill for the peak. 
However, no clear relationship between the peak intensity of the season and the skill at forecasting the peak intensity was observed (data not shown).
% See analysis in code/peak-analysis.R.

[[Add a paragraph about capturing a holiday spike?]]

% \begin{itemize}
%     \item holiday peak?
%     \item missing a slow descent because of a second outbreak that is lifting the tail up?
% \end{itemize}


\section{Discussion}

\subsection{Overview of key results and importance}
The first large-scale comparison of flu forecasting models from different modeling teams/philosophies across multiple years.

\subsection{Overview of statistical vs. mechanistic model comparison}
As our knowledge/data about the system mature, we expect mechanistic models to be better, but when true signals of mechanistic model is drowned out by observational noise or spatial aggregation, statistical models may perform better. This comparison serves as a barometer for where the current state of forecast models are.

\subsection{Limitations}

\begin{itemize}
    \item relatively few additional data sources incorporated
    \item no models that explicitly incorporate strain information
    \item no models with spatial information included
    \item seven seasons of data is not a lot (n=7) to draw strong conclusions about comparative model performance
    \item currently limited to models with only recent data...
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{../flusightnetwork.bib}

\end{document}